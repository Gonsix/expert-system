{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Remove NaN From Preprocessed Enron Datasets"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1042,"status":"ok","timestamp":1689127984791,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"lKqY6J0pdYTM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","df = pd.read_csv(\"preprocessed_enron.csv\")\n","#Remove NaN value\n","for i, msg in enumerate(df['body']):\n","    # print(i, msg)\n","    if msg is np.nan:\n","        df = df.drop(i)\n","for i, msg in enumerate(df['body']):\n","    if msg is np.nan:\n","        print(i, msg)\n","df = df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 3. allow user to select reference corpus essential"]},{"cell_type":"markdown","metadata":{},"source":["#Feature3"]},{"cell_type":"markdown","metadata":{},"source":["## User input for Chosing Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["while True:\n","    try:\n","        dataset_num = input('1: Enron+K1 Datasets, 2: Enron+K2 Datasets\\nChoose 1 or 2:')\n","        dataset_num = int(dataset_num)\n","        if dataset_num == 1 or dataset_num==2:\n","            break\n","    except EOFError as e:\n","        print(e)\n","        break\n","    dataset_num = input('1: Enron+K1 Datasets, 2: Enron+K2 Datasets\\nChoose 1 or 2:')\n","    dataset_num = dataset_num.strip() # 空白文字のみの入力は無視する\n","    if dataset_num == 1 or dataset_num==2:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["## Merge Enron Dataset with the Dataset User Chose"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["if(dataset_num == 1):\n","    f = open('K1_dataset.txt', 'r', encoding='UTF-8')\n","else:\n","    f = open('K2_dataset.txt', 'r', encoding='UTF-8')\n","K1_data = f.read()\n","df.loc[len(df)] = ['K1@dataset.com', K1_data] #add K1 datasets into the last row\n","f.close()"]},{"cell_type":"markdown","metadata":{},"source":["# 1. count, list and order the frequency of words"]},{"cell_type":"markdown","metadata":{},"source":["#Feature1"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1968,"status":"ok","timestamp":1689127987185,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"vaALdJiudYTX"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","documents = df['body'].tolist()\n","\n","# for i, msg in enumerate(df['body']):\n","#     # print(i, msg)\n","#     if msg is np.nan:\n","#         print(i, msg)\n","\n","#tfidf_vectorizer = TfidfVectorizer()\n","#tfidf_vectors = tfidf_vectorizer.fit_transform(documents) # keyword frequency list\n","tf_vectorizer = CountVectorizer()\n","tf_vectors = tf_vectorizer.fit_transform(documents)         # word frequency list\n","# del tf_vectorizer\n","# del tfidf_vectorizer# データ分割r"]},{"cell_type":"markdown","metadata":{},"source":["## Display Words and Freq"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":701,"status":"ok","timestamp":1689127987185,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"3whZTsPBrMtE","outputId":"acc484a6-0362-4557-ead1-1bd1536062c0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>zoning</th>\n","      <td>9396</td>\n","    </tr>\n","    <tr>\n","      <th>zones</th>\n","      <td>9395</td>\n","    </tr>\n","    <tr>\n","      <th>zone</th>\n","      <td>9394</td>\n","    </tr>\n","    <tr>\n","      <th>znamenny</th>\n","      <td>9393</td>\n","    </tr>\n","    <tr>\n","      <th>zivic</th>\n","      <td>9392</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>005</th>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>000mmbtu</th>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>000f749c00763c4c3b9aca0044810ab2</th>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>000</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>00</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9397 rows × 1 columns</p>\n","</div>"],"text/plain":["                                  frequency\n","zoning                                 9396\n","zones                                  9395\n","zone                                   9394\n","znamenny                               9393\n","zivic                                  9392\n","...                                     ...\n","005                                       4\n","000mmbtu                                  3\n","000f749c00763c4c3b9aca0044810ab2          2\n","000                                       1\n","00                                        0\n","\n","[9397 rows x 1 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_disp_word_freq = pd.DataFrame.from_dict(data = tf_vectorizer.vocabulary_, orient=\"index\", columns=[\"frequency\"]).sort_values('frequency', ascending=False)\n","df_disp_word_freq"]},{"cell_type":"markdown","metadata":{},"source":["## Make Words List Corresponding with the Index of \"tf_vectors\""]},{"cell_type":"markdown","metadata":{},"source":["### \"tf_vectors\" represents frequency word matrix, \n","Columns of which are all words in all documents, \n","\n","Rows of which are the value of furequency words in a document."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1689127987186,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"mblTowsDdYTg","outputId":"d0e28c64-bf6a-46f4-f4b3-930bed4f76a8"},"outputs":[],"source":["# Make words dictionary and it is used in Train and Test cases\n","words=tf_vectorizer.get_feature_names_out()\n","#print(words[5020:5025])"]},{"cell_type":"markdown","metadata":{},"source":["## tf_vectors matrix"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":373,"status":"ok","timestamp":1689127987552,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"WNCL7tHudYTh"},"outputs":[],"source":["#tfidf_mat = tfidf_vectors.toarray() # dead every time\n","#del tfidf_vectors\n","tf_mat = tf_vectors.toarray()\n","del tf_vectors\n","df['tf'] = tf_mat.tolist()\n","#df['tfidf'] = tfidf_mat.tolist()"]},{"cell_type":"markdown","metadata":{},"source":["## Remove Zero Vector Record for Normalizing Token Frequency"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1689127987554,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"ZSGL6Vbkngv5"},"outputs":[],"source":["for i, vec in enumerate(df['tf']):\n","    if sum(vec) == 0:\n","        df = df.drop(i)\n","\n","df = df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Make Normalirzed Token Frequency List into df"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5392,"status":"ok","timestamp":1689127992938,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"ekr7GC_agOtq"},"outputs":[],"source":["# Normalization\n","# we generaly name 'ntf' for normalized term frequency\n","normalized_tf_list = []\n","for row in df['tf']:\n","    num_words = sum(row)\n","    normalized_tf = []\n","    for x in row:\n","        normalized_tf.append(x/num_words)\n","\n","    normalized_tf_list.append(normalized_tf)\n","df['ntf'] = normalized_tf_list"]},{"cell_type":"markdown","metadata":{"id":"TzgePT9IdscE"},"source":["#Tf-idf の代わりに利用する keyness を作る\n","\n","ここでは　df['keyness'] を作成し追加したい"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5376,"status":"ok","timestamp":1689127998292,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"d0mwa0wUdruP"},"outputs":[],"source":["import math\n","# we generaly name 'ntf' for normalized term frequency\n","\n","# First, create the shared normalized tf vector\n","shared_ntf = None # shared ntf of all document\n","\n","matrix = []\n","for row in df['ntf']:\n","    matrix.append(row)\n","\n","np_matrix = np.array(matrix)\n","\n","mean_vector = np_matrix.mean(axis=0)\n","\n","shared_ntf = mean_vector.tolist()\n","\n","def keyness(ntf_vector1, ref_ntf_vector2): # freq_vector1 and freq_vector2 are both already normalized\n","    keyness_vec = []\n","    for i, x in enumerate(ntf_vector1):\n","        if ntf_vector1[i] == 0:\n","            keyness_vec.append(0)\n","        else:\n","            keyness_vec.append(math.log2(ntf_vector1[i]/ref_ntf_vector2[i]))\n","\n","    return keyness_vec\n","\n","keyness_mat = []\n","for ntf_vector in df['ntf']:\n","    keyness_vec = keyness(ntf_vector, shared_ntf)\n","    keyness_mat.append(keyness_vec)\n","\n","# Add Keyness value matrix into df\n","df['keyness'] = keyness_mat"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689127998293,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"_jlIDOCY5Fvv","outputId":"7de19469-6314-42c3-828b-5bc1a57eedf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["mechelle\n","Mechelle,\n","            I will let you make the call, MW's or $'s. Look at the spreadsheet  below and give me your thoughts.\n"," \n","                                                              Chris\n","              \n","\n","\n"]}],"source":["#確認用\n","i = 22\n","msg = df['body'][i]\n","max_value = max(keyness_mat[i])\n","max_idx = keyness_mat[i].index(max_value)\n","print(words[max_idx])\n","\n","print(msg)\n","#print(df['author'][i])\n"]},{"cell_type":"markdown","metadata":{"id":"saGlZu9odYTi"},"source":["## Split data into Train Data and Test Data"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1689127998710,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"-UZnYWEVdYTj"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_keyness_train, X_keyness_test, Y_keyness_train, Y_keyness_test = train_test_split(df['keyness'],df['author'],test_size=0.2,shuffle=True)\n","X_tf_train, X_tf_test, Y_tf_train, Y_tf_test = train_test_split(df['tf'],df['author'],test_size=0.2,shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"fDqdukz0yGIl"},"source":["## Check the number of the Authors"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689127998711,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"a5lkXXfDdYTl"},"outputs":[],"source":["# How many author?\n","authors = set(Y_keyness_test)\n","authors_list = [author for author in authors]"]},{"cell_type":"markdown","metadata":{"id":"aedNBJK4dYTm"},"source":["## Create Reference_vectors\n","size: the number of author\n","\n","This is made of Train data"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3485,"status":"ok","timestamp":1689128002192,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"pgdwL7B0dYTn"},"outputs":[],"source":["# df_X = pd.DataFrame(X_tf_train.values.tolist())\n","# df_Y = pd.DataFrame(Y_tf_train.values.tolist())\n","\n","\n","df_train = pd.concat((X_keyness_train, Y_keyness_train.rename('author')), axis=1)\n","#\n","reference_vectors = {}\n","for author in authors:\n","\n","    df_author = df_train.groupby('author').get_group(author)\n","\n","    matrix = []\n","    for row in df_author['keyness']:\n","        matrix.append(row)\n","\n","    np_matrix = np.array(matrix)\n","\n","    mean_vector = np_matrix.mean(axis=0)\n","    reference_vectors[author] = mean_vector.tolist()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1689128002193,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"KRE3PFpedYTo","outputId":"11cd3b21-8ef1-4410-e179-79ef410a0ec4"},"outputs":[{"name":"stdout","output_type":"stream","text":["andy.zipper@enron.com: andy\n","ben.jacoby@enron.com: ben\n","chris.stokley@enron.com: chris\n","v.weldon@enron.com: charlie\n","j..kean@enron.com: see\n","hunter.shively@enron.com: hunter\n"]}],"source":["for author, ref_vec in reference_vectors.items():\n","    max_value = max(ref_vec)\n","    max_idx =ref_vec.index(max_value)\n","    print(f'{author}: {words[max_idx]}')\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1689128002194,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"NubkriNNdYTp","outputId":"1aad6f3a-390c-4bd7-a572-f8e18e79b751"},"outputs":[{"data":{"text/plain":["{'andy.zipper@enron.com',\n"," 'ben.jacoby@enron.com',\n"," 'chris.stokley@enron.com',\n"," 'hunter.shively@enron.com',\n"," 'j..kean@enron.com',\n"," 'v.weldon@enron.com'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["authors"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1689128002506,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"5c9DcF4GdYTq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1689128002507,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"r6g6FXopdYTu"},"outputs":[],"source":["# import nltk\n","# from nltk.corpus import stopwords\n","\n","# nltk.download('stopwords')\n","# stop_words = stopwords.words('english')"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1689128002508,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"byj_PcU4dYTz"},"outputs":[],"source":["# start からend までのwordの配列を返す\n","def extract_features_words(freq_vector, words, start=0, end=20):\n","\n","    setX = set(freq_vector) # 最大値を取り出すため set を作成\n","\n","    count = 0\n","\n","    result = []\n","\n","    while count<end:\n","        max_value = max(setX)\n","        max_index = freq_vector.index(max_value)\n","        max_word = words[max_index]\n","\n","        setX.remove(max_value)\n","\n","        ### if exclude stopwords\n","        # if max_word not in stop_words:\n","        #     if count>= start:\n","        #         result.append(max_index)\n","        #     count+=1\n","\n","        if count>= start:\n","            result.append(max_word)\n","        count += 1\n","\n","\n","    return result"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1689128002509,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"6sUvSp9pdYT1"},"outputs":[],"source":["# start からend までのword IDの配列を返す\n","def extract_features(freq_vector, words, start=0, end=20):\n","\n","    setX = set(freq_vector) # 最大値を取り出すため set を作成\n","\n","    count = 0\n","\n","    result = []\n","\n","    while count<end:\n","        try:\n","            max_value = max(setX)\n","        except ValueError:\n","            return result\n","\n","        max_index = freq_vector.index(max_value)\n","        max_word = words[max_index]\n","\n","        setX.remove(max_value)\n","\n","        ### if exclude stopwords\n","        # if max_word not in stop_words:\n","        #     if count>= start:\n","        #         result.append(max_index)\n","        #     count+=1\n","\n","        if count>= start:\n","            result.append(max_word)\n","        count += 1\n","\n","        #\n","\n","\n","    return result"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1689128002510,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"A63Z5oPuxIyu","outputId":"340abf3e-7bef-48cd-e839-9ff19b0ad48b"},"outputs":[{"data":{"text/plain":["{'andy.zipper@enron.com',\n"," 'ben.jacoby@enron.com',\n"," 'chris.stokley@enron.com',\n"," 'hunter.shively@enron.com',\n"," 'j..kean@enron.com',\n"," 'v.weldon@enron.com'}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["authors"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1689128002511,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"u3XEgqOTdYT3","outputId":"0cbc9c22-3bbe-4637-c449-dde7ea7b1c47"},"outputs":[{"name":"stdout","output_type":"stream","text":["['see', 'attached', 'ken', 'we', 'please', 'my', 'of', 'our', 'but', 'think', 'for', 'have', 'be', 'as', 'and', 'some', 'is', 'with', 'this', 'about', 'he', 'california', 'the', 'blackberry', 'to', 'like', 'make', 'management', 'message', 'handheld', 'net', 'organizations', 'wireless', 'should', 'organization', 'from', 'are', 'it', 'fyi', 'not', 'rto', 'would', 'corporate', 'synchronizing', 'lists', 'humor', 'sent', 'was', 'his', 'in', 'so', 'buydown', 'crisis', 'communications', 'cftc', 'folder', 'press', 'india', 'steven', 'kean', 'council', 'me', 'what', 'don', 'very', 'much', 'or', 'been', 'issues', 'that', 'associations', 'profits', 'at', 'objectives', 'contacts', 'reg', 'campaign', 'you', 'advertising', 'inbox', 'presentations', 'forms', 'people', 'know', 'they', 'more', 'committee', 'meeting', 'personal', 'business', 'your', 'federal', 'revised', 'credit', 'way', 'leadership', 'brazil', 'offline', 'fine', 'security', 'mexico', 'one', 'japan', 'www', 'sec', 'will', 'him', 'better', 'contributions', 'hr', 'has', 'part', 'staff', 'demand', 'government', 'version', 'tasks', 'advisory', 'tax', 'won', 'ferc', 'electricity', 'budget', 'internal', 'voicemail', 'excess', 'aviation', 'speeches', 'per', 'pac', 'send', 'enron', 'rules', 'talked', 'call', 'wind', 'working', 'just', 'if', 'releases', 'pr', 'can', 'europe', 'restructuring', 'environmental', 'other', 'responsibility', 'compensation', 'yet', 'had']\n"]}],"source":["vec = reference_vectors['j..kean@enron.com']\n","top_words = extract_features_words(vec, words=words,  start=0, end=150)\n","print(top_words)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1689128002512,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"xWhTAjQYkwOy"},"outputs":[],"source":["def get_similarity(feature_vector1,feature_vector2):\n","    return len(set(feature_vector1) & set(feature_vector2))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eQqQb_FVxIyw"},"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1689129106975,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"BBTAic9Mdytb"},"outputs":[],"source":["\n","INF = float('inf')\n","\n","def predict(questioned_vector,reference_vectors):\n","    start = 0\n","    end = 20\n","    suspected = [author for author in authors]\n","    innocent_list = []\n","    while(len(suspected) > 1):\n","        Q_features = extract_features(questioned_vector, words, start, end)\n","        similarityWithQ = {}\n","\n","        for author, reference_vector in reference_vectors.items():\n","            if author in suspected: #\n","                feature_vector = extract_features(reference_vector, words, start, end)\n","                score = get_similarity(feature_vector,Q_features)\n","                similarityWithQ[author]=score\n","\n","        # innocent_list に含まれない著者の中から1人を選ぶ\n","\n","        innocent = min(similarityWithQ, key=similarityWithQ.get)\n","        #print(innocent)\n","        # print(type(innocent))\n","        # print(f'{min(similarityWithQ, key=similarityWithQ.get)} may be innocent.')\n","        suspected.remove(innocent)\n","        # similarityWithQ[innocent] = INF\n","\n","\n","\n","        end += 20\n","    return suspected[0]"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1689128002515,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"vmAs7ef3xIyw"},"outputs":[],"source":["# suspected = [author for author in authors]\n","# suspected.remove('andy.zipper@enron.com')\n","# suspected"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1689128002517,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"Es9F4VSjxIyx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BJVfCH7wy3LE"},"source":["## Check function in Train Data"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1689128019130,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"1PsLfZIkxIyx","outputId":"6a3aaede-d05d-42ba-a843-7e031f4f2869"},"outputs":[{"name":"stdout","output_type":"stream","text":["bad guy : v.weldon@enron.com\n","--------------------\n","True author : chris.stokley@enron.com\n"]}],"source":["i = 0\n","bad_guy = predict(df['keyness'][i], reference_vectors)\n","print(f'bad guy : {bad_guy}')\n","print('-'*20)\n","print(f\"True author : {df['author'][i]}\")"]},{"cell_type":"markdown","metadata":{"id":"3IoM1rGcdYT5"},"source":["## Check function in Test Data"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1689128141907,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"rUCFJ7JR4eUK","outputId":"35dbdb07-9633-4f1e-ca59-6170b2e3f626"},"outputs":[{"data":{"text/plain":["2392    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1593    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","110     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","63      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","                              ...                        \n","1016    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1846    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1483    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1351    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1102    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","Name: keyness, Length: 531, dtype: object"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["X_keyness_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1689128391255,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"LxYVO6uA5V8k","outputId":"46fbc858-61bf-4572-fa69-6fe488cf334e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1689129207208,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"rHK8sTcD8RSV"},"outputs":[],"source":["all_test_data = len(X_keyness_test)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341225,"status":"ok","timestamp":1689129589788,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"Q3atvZGPkAP7","outputId":"87a4553b-1602-41c0-8292-f5fb7f16cd10"},"outputs":[],"source":["match_cnt = 0\n","all_test_data = len(X_keyness_train)\n","for i in X_keyness_train.index:\n","    bad_guy = predict(df['keyness'][i], reference_vectors)\n","    if df['author'][i] == bad_guy:\n","        match_cnt = match_cnt + 1\n","\n","print(f'Math rate is: {match_cnt/all_test_data*100} % ')\n","    #print(f'bad guy : {bad_guy}')\n","    #print('-'*20)\n","    #print(f\"True author : {df['author'][i]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689128824864,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"t7ltZakCdYT6"},"outputs":[],"source":["# def predict(questioned_vector, reference_vectors):\n","#     suspected = [author for author in authors]\n","\n","#     comparedSize = 20\n","#     while(len(suspected) > 1):\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":41,"status":"aborted","timestamp":1689128002523,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"wee-RGzbdYT7"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":42,"status":"aborted","timestamp":1689128002524,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"o3thPLxAdYT9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43,"status":"aborted","timestamp":1689128002525,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"X6PHDHBmdYT-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"qsTpIbg_dYT_"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"expsystem","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
