{"cells":[{"cell_type":"markdown","metadata":{"id":"p9vEDloTOkQ1"},"source":["# 3.allow user to select reference corpus\n","\n","#Feature3"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2652,"status":"ok","timestamp":1689907909136,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"ln46tEDTOQt6","outputId":"b415042c-13c9-4b7b-827e-83324cf07c82"},"outputs":[{"name":"stdout","output_type":"stream","text":["SUCCESS: You chose: 1 Brown corpus\n"]}],"source":["dataset1 = \"Brown corpus\"\n","dataset2 = \"Enron corpus\"\n","while 1:\n","    selected_dataset = input(f'Select Courpus you want to use: \\n 1: {dataset1} 2: {dataset2}\\n')\n","    selected_dataset = int(selected_dataset)\n","    if(selected_dataset == 1 or selected_dataset ==2):\n","        exec_command = f\"print(f'SUCCESS: You chose: \" + str(selected_dataset) + \" \" + \"{dataset\" + str(selected_dataset) + \"}')\"\n","        exec(exec_command)\n","        break\n","    print('Please input decimal number\\n')\n","    try:\n","        selected_dataset = int(selected_dataset)\n","    except:\n","        print('Please input decimal number\\n')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Remove NaN and Change Enron Documents into one Document"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"TlXI0jWwYSVb"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","def rmNan(df):\n","    #remove nan\n","    for i, msg in enumerate(df['body']):\n","        # print(i, msg)\n","        if msg is np.nan:\n","            df = df.drop(i)\n","    for i, msg in enumerate(df['body']):\n","        if msg is np.nan:\n","            print(i, msg) \n","\n","    #merge documents into one\n","    str_all_document=''\n","    for index, record in df.iterrows():\n","        str_all_document = str_all_document + str(record[1])\n","    del df\n","    \n","    return pd.DataFrame({\"author\":[\"ENRON DATASET\"],\n","                        \"body\": [str_all_document]})\n","            "]},{"cell_type":"markdown","metadata":{"id":"zOvUoP9sRtVm"},"source":["## Make DataFrame, Q, K1, K2, and Enron or Brown Datasets"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689908970311,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"lKqY6J0pdYTM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def makeDataset(datasetnum = 0):\n","    if(datasetnum == 2):\n","        df = pd.read_csv(\"preprocessed_enron.csv\")\n","        df = rmNan(df)\n","    elif(datasetnum== 1):\n","        f = open('ca_ca01.txt', 'r')\n","        data = f.read()\n","        f.close()\n","        f = open('ca_ca02.txt', 'r')\n","        data = f.read() + ' ' + data\n","        f.close()\n","        f = open('ca_ca03.txt', 'r')\n","        data = f.read() + ' ' + data\n","        f.close()\n","        f = open('ca_ca04.txt', 'r')\n","        data = f.read() + ' ' + data\n","        f.close()\n","        print(data)\n","        df = pd.DataFrame({\"author\":[\"BROWN DATASET\"],\n","                          \"body\": [data]})\n","    return df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["#Make df_Q dataset \n","f = open('Q_dataset.txt', 'r')\n","data = f.read()\n","f.close()\n","df_Q = pd.DataFrame({\"author\":[\"Q DATASET\"],\n","                    \"body\": [data]})\n","                    "]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["#Make df_K1 dataset \n","f = open('K1_dataset.txt', 'r')\n","data = f.read()\n","f.close()\n","df_K1 = pd.DataFrame({\"author\":[\"K1 DATASET\"],\n","                    \"body\": [data]})\n"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["#Make df_K2 dataset \n","f = open('K2_dataset.txt', 'r')\n","data = f.read()\n","f.close()\n","df_K2 = pd.DataFrame({\"author\":[\"K2 DATASET\"],\n","                    \"body\": [data]})"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Oslo The most positive element to emerge from the Oslo meeting of North Atlantic Treaty Organization Foreign Ministers has been the freer , franker , and wider discussions , animated by much better mutual understanding than in past meetings . \n","This has been a working session of an organization that , by its very nature , can only proceed along its route step by step and without dramatic changes . \n","In Oslo , the ministers have met in a climate of candor , and made a genuine attempt to get information and understanding one another 's problems . \n","This atmosphere of understanding has been particularly noticeable where relations are concerned between the \" colonialist \" powers and those who have never , or not for a long time , had such problems . \n","The nightmare of a clash between those in trouble in Africa , exacerbated by the difficulties , changes , and tragedies facing them , and other allies who intellectually and emotionally disapprove of the circumstances that have brought these troubles about , has been conspicuous by its absence . \n","Explosion avoided In the case of Portugal , which a few weeks ago was rumored ready to walk out of the NATO Council should critics of its Angola policy prove harsh , there has been a noticeable relaxation of tension . \n","The general , remarkably courteous , explanation has left basic positions unchanged , but there has been no explosion in the council . \n","There should even be no more bitter surprises in the UN General Assembly as to NATO members ' votes , since a new ad hoc NATO committee has been set up so that in the future such topics as Angola will be discussed in advance . \n","Canada alone has been somewhat out of step with the Oslo attempt to get all the allied cars back on the track behind the NATO locomotive . \n","Even Norway , despite daily but limited manifestations against atomic arms in the heart of this northernmost capital of the alliance , is today closer to the NATO line . \n","On the negative side of the balance sheet must be set some disappointment that the United States leadership has not been as much in evidence as hoped for . \n","One diplomat described the tenor of Secretary of State Dean Rusk 's speeches as \" inconclusive \" . \n","But he hastened to add that , if United States policies were not always clear , despite Mr. Rusk 's analysis of the various global danger points and setbacks for the West , this may merely mean the new administration has not yet firmly fixed its policy . \n","Exploratory mood A certain vagueness may also be caused by tactical appreciation of the fact that the present council meeting is a semipublic affair , with no fewer than six Soviet correspondents accredited . \n","The impression has nevertheless been given during these three days , despite Mr. Rusk 's personal popularity , that the United States delegation came to Oslo in a somewhat tentative and exploratory frame of mind , more ready to listen and learn than to enunciate firm policy on a global scale with detailed application to individual danger spots . \n","The Secretary of State himself , in his first speech , gave some idea of the tremendous march of events inside and outside the United States that has preoccupied the new administration in the past four months . \n","But where the core of NATO is concerned , the Secretary of State has not only reiterated the United States ' profound attachment to the alliance , \" cornerstone \" of its foreign policy , but has announced that five nuclear submarines will eventually be at NATO 's disposal in European waters . \n","The Secretary of State has also solemnly repeated a warning to the Soviet Union that the United States will not stand for another setback in Berlin , an affirmation once again taken up by the council as a whole . \n","Conflict surveyed The secretary 's greatest achievement is perhaps the rekindling of NATO realization that East-West friction , wherever it take place around the globe , is in essence the general conflict between two entirely different societies , and must be treated as such without regard to geographical distance or lack of apparent connection . \n","The annual spring meeting has given an impetus in three main directions : more , deeper , and more timely political consultation within the alliance , the use of the Organization for Economic Cooperation and Development ( when ratified ) as a method of coordinating aid to the underdeveloped countries , and the need for strengthening conventional forces as well as the maintenance of the nuclear deterrent . \n","This increase in the \" threshold \" , as the conventional forces strengthening is called , will prove one of the alliance 's most difficult problems in the months to come . \n","Each ally will have to carry out obligations long since laid down , but never completely fulfilled . \n","Washington The Kennedy administration moves haltingly toward a Geneva conference on Laos just as serious debate over its foreign policy erupts for the first time . \n","There is little optimism here that the Communists will be any more docile at the conference table than they were in military actions on the ground in Laos . \n","The United States , State Department officials explain , now is mainly interested in setting up an international inspection system which will prevent Laos from being used as a base for Communist attacks on neighboring Thailand and South Viet Nam . \n","They count on the aid of the neutral countries attending the Geneva conference to achieve this . \n","The United States hopes that any future Lao Cabinet would not become Communist dominated . \n","But it is apparent that no acceptable formula has been found to prevent such a possibility . \n","Policies modified The inclination here is to accept a de facto cease-fire in Laos , rather than continue to insist on a verification of the cease-fire by the international control commission before participating in the Geneva conference . \n","This is another of the modifications of policy on Laos that the Kennedy administration has felt compelled to make . \n","It excuses these actions as being the chain reaction to basic errors made in the previous administration . \n","Its spokesmen insist that there has not been time enough to institute reforms in military and economic aid policies in the critical areas . \n","But with the months moving on -- and the immediate confrontations with the Communists showing no gain for the free world -- the question arises : How effective have Kennedy administration first foreign policy decisions been in dealing with Communist aggression ? ? \n","Former Vice-President Richard M. Nixon in Detroit called for a firmer and tougher policy toward the Soviet Union . \n","He was critical of what he feels is President Kennedy 's tendency to be too conciliatory . \n","GOP restrained It does not take a Gallup poll to find out that most Republicans in Congress feel this understates the situation as Republicans see it . \n","They can hardly restrain themselves from raising the question of whether Republicans , if they had been in power , would have made \" amateurish and monumental blunders \" in Cuba . \n","One Republican senator told this correspondent that he was constantly being asked why he did n't attack the Kennedy administration on this score . \n","His reply , he said , was that he agreed to the need for unity in the country now . \n","But he further said that it was better politics to let others question the wisdom of administration policies first . \n","The Republicans some weeks ago served notice through Senator Thruston B. Morton ( R ) of Kentucky , chairman of the Republican National Committee , that the Kennedy administration would be held responsible if the outcome in Laos was a coalition government susceptible of Communist domination . \n","Kennedy administration policies also have been assailed now from another direction by 70 Harvard , Boston University , Brandeis , and Massachusetts Institute of Technology educators . \n","Detente urged This group pleads with the administration to \" give no further support for the invasion of Cuba by exile groups \" . \n","It recommends that the United States \" seek instead to detach the Castro regime from the Communist bloc by working for a diplomatic detente and a resumption of trade relations ; ; and concentrate its constructive efforts on eliminating in other parts of Latin America the social conditions on which totalitarian nationalism feeds \" . \n","Mr. Nixon , for his part , would oppose intervention in Cuba without specific provocation . \n","But he did recommend that President Kennedy state clearly that if Communist countries shipped any further arms to Cuba that it would not be tolerated . \n","Until the Cuban fiasco and the Communist military victories in Laos , almost any observer would have said that President Kennedy had blended a program that respected , generally , the opinions voiced both by Mr. Nixon and the professors . \n","Aid plans revamped Very early in his administration he informed the Kremlin through diplomatic channels , a high official source disclosed , that the new administration would react even tougher than the Eisenhower administration would during the formative period of the administration . \n","Strenuous efforts were made to remove pin pricking from administration statements . \n","Policies on nuclear test ban negotiations were reviewed and changed . \n","But thus far there has been no response in kind . \n","Foreign aid programs were revamped to give greater emphasis to economic aid and to encourage political reform in recipient nations . \n","In Laos , the administration looked at the Eisenhower administration efforts to show determination by sailing a naval fleet into Southeast Asian waters as a useless gesture . \n","Again and again it asked the Communists to \" freeze \" the military situation in Laos . \n","But the Communists aided the Pathet Lao at an even faster rate . \n","And after several correspondents went into Pathet Lao territory and exposed the huge build-up , administration spokesmen acclaimed them for performing a \" great service \" and laid the matter before the Southeast Asia Treaty Organization . \n","SEATO was steamed up and prepared contingency plans for coping with the military losses in Laos . \n","But the Communists never gave sufficient provocation at any one time for the United States to want to risk a limited or an all-out war over Laos . \n","( Some SEATO nations disagreed , however . \n",") There was the further complication that the administration had very early concluded that Laos was ill suited to be an ally , unlike its more determined neighbors , Thailand and South Viet Nam . \n","The administration declared itself in favor of a neutralized Laos . \n","The pro-Western government , which the United States had helped in a revolt against the Souvanna Phouma \" neutralist \" government , never did appear to spark much fighting spirit in the Royal Lao Army . \n","There certainly was not any more energy displayed after it was clear the United States would not back the pro-Western government to the hilt . \n","If the administration ever had any ideas that it could find an acceptable alternative to Prince Souvanna Phouma , whom it felt was too trusting of Communists , it gradually had to relinquish them . \n","One factor was the statement of Senator J.W. Fulbright ( D ) of Arkansas , chairman of the Senate Foreign Relations Committee . \n","He declared on March 25 that the United States had erred a year and a half ago by \" encouraging the removal \" of Prince Souvanna . \n","Washington The White House is taking extraordinary steps to check the rapid growth of juvenile delinquency in the United States . \n","The President is deeply concerned over this problem and its effect upon the \" vitality of the nation \" . \n","In an important assertion of national leadership in this field , he has issued an executive order establishing the President 's committee on Juvenile Delinquency and Crime , to be supported and assisted by a Citizens Advisory Council of recognized authorities on juvenile problems . \n","The President asks the support and cooperation of Congress in his efforts through the enactment of legislation to provide federal grants to states for specified efforts in combating this disturbing crime trend . \n","Offenses multiply The President has also called upon the Attorney General , the Secretary of Health , Education and Welfare , and the Secretary of Labor to coordinate their efforts \" in the development of a program of federal leadership to assist states and local communities in their efforts to cope with the problem . \n","Simultaneously the President announced Thursday the appointment of David L. Hackett , a special assistant to the Attorney General , as executive director of the new Committee on Juvenile Delinquency and Youth Crime . \n","His sense of urgency in this matter stems from the fact that court cases and juvenile arrests have more than doubled since 1948 , each year showing an increase in offenders . \n","Among arrests reported by the Federal Bureau of Investigation in 1959 , about half for burglary and larceny involved persons under 18 years of age . \n"," Several defendants in the Summerdale police burglary trial made statements indicating their guilt at the time of their arrest , Judge James B. Parsons was told in Criminal court yesterday . \n","The disclosure by Charles Bellows , chief defense counsel , startled observers and was viewed as the prelude to a quarrel between the six attorneys representing the eight former policemen now on trial . \n","Bellows made the disclosure when he asked Judge Parsons to grant his client , Alan Clements , 30 , a separate trial . \n","Bellows made the request while the all-woman jury was out of the courtroom . \n","Fears prejudicial aspects \" The statements may be highly prejudicial to my client \" , Bellows told the court . \n","\" Some of the defendants strongly indicated they knew they were receiving stolen property . \n","It is impossible to get a fair trial when some of the defendants made statements involving themselves and others \" . \n","Judge Parsons leaned over the bench and inquired , \" You mean some of the defendants made statements admitting this \" ? ? \n","\" Yes , your honor \" , replied Bellows . \n","\" What this amounts to , if true , is that there will be a free-for-all fight in this case . \n","There is a conflict among the defendants \" . \n","Washington , July 24 -- President Kennedy today pushed aside other White House business to devote all his time and attention to working on the Berlin crisis address he will deliver tomorrow night to the American people over nationwide television and radio . \n","The President spent much of the week-end at his summer home on Cape Cod writing the first drafts of portions of the address with the help of White House aids in Washington with whom he talked by telephone . \n","Shortly after the Chief Executive returned to Washington in midmorning from Hyannis Port , Mass. , a White House spokesman said the address text still had \" quite a way to go \" toward completion . \n","Decisions are made Asked to elaborate , Pierre Salinger , White House press secretary , replied , \" I would say it 's got to go thru several more drafts \" . \n","Salinger said the work President Kennedy , advisers , and members of his staff were doing on the address involved composition and wording , rather than last minute decisions on administration plans to meet the latest Berlin crisis precipitated by Russia 's demands and proposals for the city . \n","The last 10 cases in the investigation of the Nov. 8 election were dismissed yesterday by Acting Judge John M. Karns , who charged that the prosecution obtained evidence \" by unfair and fundamentally illegal means \" . \n","Karns said that the cases involved a matter \" of even greater significance than the guilt or innocence \" of the 50 persons . \n","He said evidence was obtained \" in violation of the legal rights of citizens \" . \n","Karns ' ruling pertained to eight of the 10 cases . \n","In the two other cases he ruled that the state had been \" unable to make a case \" . \n","Contempt proceedings originally had been brought against 677 persons in 133 precincts by Morris J. Wexler , special prosecutor . \n","Issue jury subpoenas Wexler admitted in earlier court hearings that he issued grand jury subpenas to about 200 persons involved in the election investigation , questioned the individuals in the Criminal courts building , but did not take them before the grand jury . \n","Mayer Goldberg , attorney for election judges in the 58th precinct of the 23d ward , argued this procedure constituted intimidation . \n","Wexler has denied repeatedly that coercion was used in questioning . \n","Karns said it was a \" wrongful act \" for Wexler to take statements \" privately and outside of the grand jury room \" . \n","He said this constituted a \" very serious misuse \" of the Criminal court processes . \n","\" Actually , the abuse of the process may have constituted a contempt of the Criminal court of Cook county , altho vindication of the authority of that court is not the function of this court \" , said Karns , who is a City judge in East St. Louis sitting in Cook County court . \n","Faced seven cases Karns had been scheduled this week to hear seven cases involving 35 persons . \n","Wexler had charged the precinct judges in these cases with \" complementary \" miscount of the vote , in which votes would be taken from one candidate and given to another . \n","The cases involved judges in the 33d , 24th , and 42d precincts of the 31st ward , the 21st and 28th precincts of the 29th ward , the 18th precinct of the 4th ward , and the 9th precinct of the 23d ward . \n","The case of the judges in the 58th precinct of the 23d ward had been heard previously and taken under advisement by Karns . \n","Two other cases also were under advisement . \n","Claims precedent lacking After reading his statement discharging the 23d ward case , Karns told Wexler that if the seven cases scheduled for trial also involved persons who had been subpenaed , he would dismiss them . \n","Washington , Feb. 9 -- President Kennedy today proposed a mammoth new medical care program whereby social security taxes on 70 million American workers would be raised to pay the hospital and some other medical bills of 14.2 million Americans over 65 who are covered by social security or railroad retirement programs . \n","The President , in a special message to Congress , tied in with his aged care plan requests for large federal grants to finance medical and dental scholarships , build 20 new medical and 20 new dental schools , and expand child health care and general medical research . \n","The aged care plan , similar to one the President sponsored last year as a senator , a fight on Capitol hill . \n","It was defeated in Congress last year . \n","Cost up to $ 37 a year It would be financed by boosting the social security payroll tax by as much as $ 37 a year for each of the workers now paying such taxes . \n","The social security payroll tax is now 6 per cent -- 3 per cent on each worker and employer -- on the first $ 4,800 of pay per year . \n","The Kennedy plan alone would boost the base to $ 5,000 a year and the payroll tax to 6.5 per cent -- 3.25 per cent each . \n","Similar payroll tax boosts would be imposed on those under the railroad retirement system . \n","The payroll tax would actually rise to 7.5 per cent starting Jan. 1 , 1963 , if the plan is approved , because the levy is already scheduled to go up by 1 per cent on that date to pay for other social security costs . \n","Outlays would increase Officials estimated the annual tax boost for the medical plan would amount to 1.5 billion dollars and that medical benefits paid out would run 1 billion or more in the first year , 1963 . \n","Both figures would go higher in later years . \n","Other parts of the Kennedy health plan would entail federal grants of 750 million to 1 billion dollars over the next 10 years . \n","These would be paid for out of general , not payroll , taxes . \n","Nursing home care The aged care plan carries these benefits for persons over 65 who are under the social security and railroad retirement systems : 1 Full payment of hospital bills for stays up to 90 days for each illness , except that the patient would pay $ 10 a day of the cost for the first nine days . \n","2 Full payment of nursing home bills for up to 180 days following discharge from a hospital . \n","A patient could receive up to 300 days paid-for nursing home care under a \" unit formula \" allowing more of such care for those who use none or only part of the hospital-care credit . \n","3 Hospital outpatient clinic diagnostic service for all costs in excess of $ 20 a patient . \n","4 Community visiting nurse services at home for up to 240 days an illness . \n","The President noted that Congress last year passed a law providing grants to states to help pay medical bills of the needy aged . \n","Calls proposal modest He said his plan is designed to \" meet the needs of those millions who have no wish to receive care at the taxpayers ' expense , but who are nevertheless staggered by the drain on their savings -- or those of their children -- caused by an extended hospital stay \" . \n","\" This is a very modest proposal cut to meet absolutely essential needs \" , he said , \" and with sufficient ' deductible ' requirements to discourage any malingering or unnecessary overcrowding of our hospitals . \n","\" This is not a program of socialized medicine . \n","It is a program of prepayment of health costs with absolute freedom of choice guaranteed . \n","Every person will choose his own doctor and hospital \" . \n","Would n't pay doctors The plan does not cover doctor bills . \n","They would still be paid by the patient . \n","Apart from the aged care plan the President 's most ambitious and costly proposals were for federal scholarships , and grants to build or enlarge medical and dental schools . \n","The President said the nation 's 92 medical and 47 dental schools can not now handle the student load needed to meet the rising need for health care . \n","Moreover , he said , many qualified young people are not going into medicine and dentistry because they ca n't afford the schooling costs . \n","Contributions to schools The scholarship plan would provide federal contributions to each medical and dental school equal to $ 1,500 a year for one-fourth of the first year students . \n","The schools could use the money to pay 4-year scholarships , based on need , of up to $ 2,000 a year per student . \n","In addition , the government would pay a $ 1,000 \" cost of education \" grant to the schools for each $ 1,500 in scholarship grants . \n","Officials estimated the combined programs would cost 5.1 million dollars the first year and would go up to 21 millions by 1966 . \n","The President recommended federal \" matching grants \" totaling 700 million dollars in 10 years for constructing new medical and dental schools or enlarging the capacity of existing ones . \n","More for nursing homes In the area of \" community health services \" , the President called for doubling the present 10 million dollar a year federal grants for nursing home construction . \n","He asked for another 10 million dollar \" initial \" appropriation for \" stimulatory grants \" to states to improve nursing homes . \n","He further proposed grants of an unspecified sum for experimental hospitals . \n","In the child health field , the President said he will recommend later an increase in funds for programs under the children 's bureau . \n","He also asked Congress to approve establishment of a national child health institute . \n","Asks research funds The President said he will ask Congress to increase grants to states for vocational rehabilitation . \n","He did not say by how much . \n","For medical research he asked a 20 million dollar a year increase , from 30 to 50 millions , in matching grants for building research facilities . \n","The President said he will also propose increasing , by an unspecified amount , the 540 million dollars in the 1961-62 budget for direct government research in medicine . \n","The President said his proposals combine the \" indispensable elements in a sound health program -- people , knowledge , services , facilities , and the means to pay for them \" . \n","Reaction as expected Congressional reaction to the message was along expected lines . \n","Legislators who last year opposed placing aged-care under the social security system criticized the President 's plan . \n","Those who backed a similar plan last year hailed the message . \n","Senate Republican Leader Dirksen ( Ill. ) and House Republican Leader Charles Halleck ( Ind. ) said the message did not persuade them to change their opposition to compulsory medical insurance . \n","Halleck said the voluntary care plan enacted last year should be given a fair trial first . \n","House Speaker Sam Rayburn ( D. , Tex. ) called the Kennedy program \" a mighty fine thing \" , but made no prediction on its fate in the House . \n","Washington , Feb. 9 -- Acting hastily under White House pressure , the Senate tonight confirmed Robert C. Weaver as the nation 's federal housing chief . \n","Only 11 senators were on the floor and there was no record vote . \n","A number of scattered \" ayes \" and \" noes \" was heard . \n","Customary Senate rules were ignored in order to speed approval of the Negro leader as administrator of the housing and home finance agency . \n","In the last eight years , all Presidential appointments , including those of cabinet rank , have been denied immediate action because of a Senate rule requiring at least a 24 hour delay after they are reported to the floor . \n","Enforce by demand The rule was enforced by demand of Sen. Wayne Morse ( D. , Ore. ) in connection with President Eisenhower 's cabinet selections in 1953 and President Kennedy 's in 1961 . \n"," Austin , Texas -- Committee approval of Gov. Price Daniel 's \" abandoned property \" act seemed certain Thursday despite the adamant protests of Texas bankers . \n","Daniel personally led the fight for the measure , which he had watered down considerably since its rejection by two previous Legislatures , in a public hearing before the House Committee on Revenue and Taxation . \n","Under committee rules , it went automatically to a subcommittee for one week . \n","But questions with which committee members taunted bankers appearing as witnesses left little doubt that they will recommend passage of it . \n","Daniel termed \" extremely conservative \" his estimate that it would produce 17 million dollars to help erase an anticipated deficit of 63 million dollars at the end of the current fiscal year next Aug. 31 . \n","He told the committee the measure would merely provide means of enforcing the escheat law which has been on the books \" since Texas was a republic \" . \n","It permits the state to take over bank accounts , stocks and other personal property of persons missing for seven years or more . \n","The bill , which Daniel said he drafted personally , would force banks , insurance firms , pipeline companies and other corporations to report such property to the state treasurer . \n","The escheat law can not be enforced now because it is almost impossible to locate such property , Daniel declared . \n","Dewey Lawrence , a Tyler lawyer representing the Texas Bankers Association , sounded the opposition keynote when he said it would force banks to violate their contractual obligations with depositors and undermine the confidence of bank customers . \n","\" If you destroy confidence in banks , you do something to the economy \" , he said . \n","\" You take out of circulation many millions of dollars \" . \n","Rep. Charles E. Hughes of Sherman , sponsor of the bill , said a failure to enact it would amount \" to making a gift out of the taxpayers ' pockets to banks , insurance and pipeline companies \" . \n","His contention was denied by several bankers , including Scott Hudson of Sherman , Gaynor B. Jones of Houston , J.B. Brady of Harlingen and Howard Cox of Austin . \n","Cox argued that the bill is \" probably unconstitutional \" since , he said , it would impair contracts . \n","He also complained that not enough notice was given on the hearing , since the bill was introduced only last Monday . \n","Austin , Texas -- Senators unanimously approved Thursday the bill of Sen. George Parkhouse of Dallas authorizing establishment of day schools for the deaf in Dallas and the four other largest counties . \n","The bill is designed to provide special schooling for more deaf students in the scholastic age at a reduced cost to the state . \n","There was no debate as the Senate passed the bill on to the House . \n","It would authorize the Texas Education Agency to establish county-wide day schools for the deaf in counties of 300,000 or more population , require deaf children between 6 and 13 years of age to attend the day schools , permitting older ones to attend the residential Texas School for the Deaf here . \n","Operating budget for the day schools in the five counties of Dallas , Harris , Bexar , Tarrant and El Paso would be $ 451,500 , which would be a savings of $ 157,460 yearly after the first year 's capital outlay of $ 88,000 was absorbed , Parkhouse told the Senate . \n","The TEA estimated there would be 182 scholastics to attend the day school in Dallas County , saving them from coming to Austin to live in the state deaf school . \n","Dallas may get to hear a debate on horse race parimutuels soon between Reps. V.E. ( Red ) Berry and Joe Ratcliff . \n","While details are still to be worked out , Ratcliff said he expects to tell home folks in Dallas why he thinks Berry 's proposed constitutional amendment should be rejected . \n","\" We 're getting more ' pro ' letters than ' con ' on horse race betting \" , said Ratcliff . \n","\" But I believe if people were better informed on this question , most of them would oppose it also . \n","I 'm willing to stake my political career on it \" . \n","Rep. Berry , an ex-gambler from San Antonio , got elected on his advocacy of betting on the ponies . \n","A House committee which heard his local option proposal is expected to give it a favorable report , although the resolution faces hard sledding later . \n","The house passed finally , and sent to the Senate , a bill extending the State Health Department 's authority to give planning assistance to cities . \n","The senate quickly whipped through its meager fare of House bills approved by committees , passing the three on the calendar . \n","One validated acts of school districts . \n","Another enlarged authority of the Beaumont Navigation District . \n","The third amended the enabling act for creation of the Lamar county Hospital District , for which a special constitutional amendment previously was adopted . \n","Without dissent , senators passed a bill by Sen. A.R. Schwartz of Galveston authorizing establishment in the future of a school for the mentally retarded in the Gulf Coast district . \n","Money for its construction will be sought later on but in the meantime the State Hospital board can accept gifts and donations of a site . \n","Two tax revision bills were passed . \n","One , by Sen. Louis Crump of San Saba , would aid more than 17,000 retailers who pay a group of miscellaneous excise taxes by eliminating the requirement that each return be notarized . \n","Instead , retailers would sign a certificate of correctness , violation of which would carry a penalty of one to five years in prison , plus a $ 1,000 fine . \n","It was one of a series of recommendations by the Texas Research League . \n","The other bill , by Sen. A.M. Aikin Jr. of Paris , would relieve real estate brokers , who pay their own annual licensing fee , from the $ 12 annual occupation license on brokers in such as stocks and bonds . \n","Natural gas public utility companies would be given the right of eminent domain , under a bill by Sen. Frank Owen 3 , of El Paso , to acquire sites for underground storage reservoirs for gas . \n","Marshall Formby of Plainview , former chairman of the Texas Highway Commission , suggested a plan to fill by appointment future vacancies in the Legislature and Congress , eliminating the need for costly special elections . \n","Under Formby 's plan , an appointee would be selected by a board composed of the governor , lieutenant governor , speaker of the House , attorney general and chief justice of the Texas Supreme Court . \n","Austin , Texas -- State representatives decided Thursday against taking a poll on what kind of taxes Texans would prefer to pay . \n","An adverse vote of 81 to 65 kept in the State Affairs Committee a bill which would order the referendum on the April 4 ballot , when Texas votes on a U.S. senator . \n","Rep. Wesley Roberts of Seminole , sponsor of the poll idea , said that further delay in the committee can kill the bill . \n","The West Texan reported that he had finally gotten Chairman Bill Hollowell of the committee to set it for public hearing on Feb. 22 . \n","The proposal would have to receive final legislative approval , by two-thirds majorities , before March 1 to be printed on the April 4 ballot , Roberts said . \n","Opponents generally argued that the ballot could n't give enough information about tax proposals for the voters to make an intelligent choice . \n","All Dallas members voted with Roberts , except Rep. Bill Jones , who was absent . \n","Austin , Texas -- Paradise lost to the alleged water needs of Texas ' big cities Thursday . \n","Rep. James Cotten of Weatherford insisted that a water development bill passed by the Texas House of Representatives was an effort by big cities like Dallas and Fort Worth to cover up places like Paradise , a Wise County hamlet of 250 people . \n","When the shouting ended , the bill passed , 114 to 4 , sending it to the Senate , where a similar proposal is being sponsored by Sen. George Parkhouse of Dallas . \n","Most of the fire was directed by Cotten against Dallas and Sen. Parkhouse . \n","The bill would increase from $ 5,000,000 to $ 15,000,000 the maximum loan the state could make to a local water project . \n","Cotten construed this as a veiled effort by Parkhouse to help Dallas and other large cities get money which Cotten felt could better be spent providing water for rural Texas . \n","Statements by other legislators that Dallas is paying for all its water program by local bonds , and that less populous places would benefit most by the pending bill , did not sway Cotten 's attack . \n","The bill 's defenders were mostly small-town legislators like J.W. Buchanan of Dumas , Eligio ( Kika ) De La Garza of Mission , Sam F. Collins of Newton and Joe Chapman of Sulphur Springs . \n","\" This is a poor boy 's bill \" , said Chapman . \n","\" Dallas and Fort Worth can vote bonds . \n","This would help the little peanut districts \" . \n","Austin , Texas -- A Houston teacher , now serving in the Legislature , proposed Thursday a law reducing the time spent learning \" educational methods \" . \n","Rep. Henry C. Grover , who teaches history in the Houston public schools , would reduce from 24 to 12 semester hours the so-called \" teaching methods \" courses required to obtain a junior or senior high school teaching certificate . \n","A normal year 's work in college is 30 semester hours . \n","Grover also would require junior-senior high teachers to have at least 24 semester hours credit in the subject they are teaching . \n","The remainder of the 4-year college requirement would be in general subjects . \n","\" A person with a master 's degree in physics , chemistry , math or English , yet who has not taken Education courses , is not permitted to teach in the public schools \" , said Grover . \n","College teachers in Texas are not required to have the Education courses . \n","Fifty-three of the 150 representatives immediately joined Grover as co-signers of the proposal . \n","Paris , Texas ( sp . \n",") -- The board of regents of Paris Junior College has named Dr. Clarence Charles Clark of Hays , Kan. as the school 's new president . \n","Dr. Clark will succeed Dr. J.R. McLemore , who will retire at the close of the present school term . \n","Dr. Clark holds an earned Doctor of Education degree from the University of Oklahoma . \n","He also received a Master of Science degree from Texas A & I College and a Bachelor of Science degree from Southwestern State College , Weatherford , Okla .. In addition , Dr. Clark has studied at Rhode Island State College and Massachusetts Institute of Technology . \n","During his college career , Dr. Clark was captain of his basketball team and was a football letterman . \n","Dr. Clark has served as teacher and principal in Oklahoma high schools , as teacher and athletic director at Raymondville , Texas , High School , as an instructor at the University of Oklahoma , and as an associate professor of education at Fort Hays , Kan. , State College . \n","He has served as a border patrolman and was in the Signal Corps of the U.S. Army . \n","Denton , Texas ( sp . \n",") -- Principals of the 13 schools in the Denton Independent School District have been re-elected for the 1961-62 session upon the recommendation of Supt. Chester O. Strickland . \n","State and federal legislation against racial discrimination in employment was called for yesterday in a report of a \" blue ribbon \" citizens committee on the aid to dependent children program . \n","The report , culminating a year long study of the ADC program in Cook county by a New York City welfare consulting firm , listed 10 long range recommendations designed to reduce the soaring ADC case load . \n","The report called racial discrimination in employment \" one of the most serious causes of family breakdown , desertion , and ADC dependency \" . \n","\" Must solve problem \" The monthly cost of ADC to more than 100,000 recipients in the county is 4.4 million dollars , said C. Virgil Martin , president of Carson Pirie Scott & Co. , committee chairman . \n","\" We must solve the problems which have forced these people to depend upon ADC for subsistence \" , Martin said . \n","The volume of ADC cases will decrease , Martin reported , when the community is able to deal effectively with two problems : Relatively limited skills and discrimination in employment because of color . \n","These , he said , are \" two of the principal underlying causes for family breakups leading to ADC \" . \n","Calls for extension Other recommendations made by the committee are : Extension of the ADC program to all children in need living with any relatives , including both parents , as a means of preserving family unity . \n","Research projects as soon as possible on the causes and prevention of dependency and illegitimacy . \n"," The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced \" no evidence \" that any irregularities took place . \n","The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , \" deserves the praise and thanks of the City of Atlanta \" for the manner in which the election was conducted . \n","The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible \" irregularities \" in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr .. \" Only a relative handful of such reports was received \" , the jury said , \" considering the widespread interest in the election , the number of voters and the size of this city \" . \n","The jury said it did find that many of Georgia 's registration and election laws \" are outmoded or inadequate and often ambiguous \" . \n","It recommended that Fulton legislators act \" to have these laws studied and revised to the end of modernizing and improving them \" . \n","The grand jury commented on a number of other topics , among them the Atlanta and Fulton County purchasing departments which it said \" are well operated and follow generally accepted practices which inure to the best interest of both governments \" . \n","Merger proposed However , the jury said it believes \" these two offices should be combined to achieve greater efficiency and reduce the cost of administration \" . \n","The City Purchasing Department , the jury said , \" is lacking in experienced clerical personnel as a result of city personnel policies \" . \n","It urged that the city \" take steps to remedy \" this problem . \n","Implementation of Georgia 's automobile title law was also recommended by the outgoing jury . \n","It urged that the next Legislature \" provide enabling funds and re-set the effective date so that an orderly implementation of the law may be effected \" . \n","The grand jury took a swipe at the State Welfare Department 's handling of federal funds granted for child welfare services in foster homes . \n","\" This is one of the major items in the Fulton County general assistance program \" , the jury said , but the State Welfare Department \" has seen fit to distribute these funds through the welfare departments of all counties in the state with the exception of Fulton County , which receives none of this money . \n","The jurors said they realize \" a proportionate distribution of these funds might disable this program in our less populous counties \" . \n","Nevertheless , \" we feel that in the future Fulton County should receive some portion of these available funds \" , the jurors said . \n","\" Failure to do this will continue to place a disproportionate burden \" on Fulton taxpayers . \n","The jury also commented on the Fulton ordinary 's court which has been under fire for its practices in the appointment of appraisers , guardians and administrators and the awarding of fees and compensation . \n","Wards protected The jury said it found the court \" has incorporated into its operating procedures the recommendations \" of two previous grand juries , the Atlanta Bar Association and an interim citizens committee . \n","\" These actions should serve to protect in fact and in effect the court 's wards from undue costs and its appointed and elected servants from unmeritorious criticisms \" , the jury said . \n","Regarding Atlanta 's new multi-million-dollar airport , the jury recommended \" that when the new management takes charge Jan. 1 the airport be operated in a manner that will eliminate political influences \" . \n","The jury did not elaborate , but it added that \" there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable \" . \n","Ask jail deputies On other matters , the jury recommended that : ( 1 ) Four additional deputies be employed at the Fulton County Jail and \" a doctor , medical intern or extern be employed for night and weekend duty at the jail \" . \n","( 2 ) Fulton legislators \" work with city officials to pass enabling legislation that will permit the establishment of a fair and equitable \" pension plan for city employes . \n","The jury praised the administration and operation of the Atlanta Police Department , the Fulton Tax Commissioner 's Office , the Bellwood and Alpharetta prison farms , Grady Hospital and the Fulton Health Department . \n","Mayor William B. Hartsfield filed suit for divorce from his wife , Pearl Williams Hartsfield , in Fulton Superior Court Friday . \n","His petition charged mental cruelty . \n","The couple was married Aug. 2 , 1913 . \n","They have a son , William Berry Jr. , and a daughter , Mrs. J.M. Cheshire of Griffin . \n","Attorneys for the mayor said that an amicable property settlement has been agreed upon . \n","The petition listed the mayor 's occupation as \" attorney \" and his age as 71 . \n","It listed his wife 's age as 74 and place of birth as Opelika , Ala .. The petition said that the couple has not lived together as man and wife for more than a year . \n","The Hartsfield home is at 637 E. Pelham Rd. A[fj] . \n","Henry L. Bowden was listed on the petition as the mayor 's attorney . \n","Hartsfield has been mayor of Atlanta , with exception of one brief interlude , since 1937 . \n","His political career goes back to his election to city council in 1923 . \n","The mayor 's present term of office expires Jan. 1 . \n","He will be succeeded by Ivan Allen Jr. , who became a candidate in the Sept. 13 primary after Mayor Hartsfield announced that he would not run for reelection . \n","Georgia Republicans are getting strong encouragement to enter a candidate in the 1962 governor 's race , a top official said Wednesday . \n","Robert Snodgrass , state GOP chairman , said a meeting held Tuesday night in Blue Ridge brought enthusiastic responses from the audience . \n","State Party Chairman James W. Dorsey added that enthusiasm was picking up for a state rally to be held Sept. 8 in Savannah at which newly elected Texas Sen. John Tower will be the featured speaker . \n","In the Blue Ridge meeting , the audience was warned that entering a candidate for governor would force it to take petitions out into voting precincts to obtain the signatures of registered voters . \n","Despite the warning , there was a unanimous vote to enter a candidate , according to Republicans who attended . \n","When the crowd was asked whether it wanted to wait one more term to make the race , it voted no -- and there were no dissents . \n","The largest hurdle the Republicans would have to face is a state law which says that before making a first race , one of two alternative courses must be taken : 1 Five per cent of the voters in each county must sign petitions requesting that the Republicans be allowed to place names of candidates on the general election ballot , or 2 The Republicans must hold a primary under the county unit system -- a system which the party opposes in its platform . \n","Sam Caldwell , State Highway Department public relations director , resigned Tuesday to work for Lt. Gov. Garland Byrd 's campaign . \n","Caldwell 's resignation had been expected for some time . \n","He will be succeeded by Rob Ledford of Gainesville , who has been an assistant more than three years . \n","When the gubernatorial campaign starts , Caldwell is expected to become a campaign coordinator for Byrd . \n","The Georgia Legislature will wind up its 1961 session Monday and head for home -- where some of the highway bond money it approved will follow shortly . \n","Before adjournment Monday afternoon , the Senate is expected to approve a study of the number of legislators allotted to rural and urban areas to determine what adjustments should be made . \n","Gov. Vandiver is expected to make the traditional visit to both chambers as they work toward adjournment . \n","Vandiver likely will mention the $ 100 million highway bond issue approved earlier in the session as his first priority item . \n","Construction bonds Meanwhile , it was learned the State Highway Department is very near being ready to issue the first $ 30 million worth of highway reconstruction bonds . \n","The bond issue will go to the state courts for a friendly test suit to test the validity of the act , and then the sales will begin and contracts let for repair work on some of Georgia 's most heavily traveled highways . \n","A Highway Department source said there also is a plan there to issue some $ 3 million to $ 4 million worth of Rural Roads Authority bonds for rural road construction work . \n","A revolving fund The department apparently intends to make the Rural Roads Authority a revolving fund under which new bonds would be issued every time a portion of the old ones are paid off by tax authorities . \n","Vandiver opened his race for governor in 1958 with a battle in the Legislature against the issuance of $ 50 million worth of additional rural roads bonds proposed by then Gov. Marvin Griffin . \n","The Highway Department source told The Constitution , however , that Vandiver has not been consulted yet about the plans to issue the new rural roads bonds . \n","Schley County Rep. B.D. Pelham will offer a resolution Monday in the House to rescind the body 's action of Friday in voting itself a $ 10 per day increase in expense allowances . \n","Pelham said Sunday night there was research being done on whether the \" quickie \" vote on the increase can be repealed outright or whether notice would have to first be given that reconsideration of the action would be sought . \n","While emphasizing that technical details were not fully worked out , Pelham said his resolution would seek to set aside the privilege resolution which the House voted through 87-31 . \n","A similar resolution passed in the Senate by a vote of 29-5 . \n","As of Sunday night , there was no word of a resolution being offered there to rescind the action . \n","Pelham pointed out that Georgia voters last November rejected a constitutional amendment to allow legislators to vote on pay raises for future Legislature sessions . \n","A veteran Jackson County legislator will ask the Georgia House Monday to back federal aid to education , something it has consistently opposed in the past . \n","Rep. Mac Barber of Commerce is asking the House in a privilege resolution to \" endorse increased federal support for public education , provided that such funds be received and expended \" as state funds . \n","Barber , who is in his 13th year as a legislator , said there \" are some members of our congressional delegation in Washington who would like to see it ( the resolution ) passed \" . \n","But he added that none of Georgia 's congressmen specifically asked him to offer the resolution . \n","The resolution , which Barber tossed into the House hopper Friday , will be formally read Monday . \n","It says that \" in the event Congress does provide this increase in federal funds \" , the State Board of Education should be directed to \" give priority \" to teacher pay raises . \n","Colquitt -- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the \" coolest election I ever saw in this county \" . \n","The new school superintendent is Harry Davis , a veteran agriculture teacher , who defeated Felix Bush , a school principal and chairman of the Miller County Democratic Executive Committee . \n","Davis received 1,119 votes in Saturday 's election , and Bush got 402 . \n","Ordinary Carey Williams , armed with a pistol , stood by at the polls to insure order . \n","\" This was the coolest , calmest election I ever saw \" , Colquitt Policeman Tom Williams said . \n","\" Being at the polls was just like being at church . \n","I did n't smell a drop of liquor , and we did n't have a bit of trouble \" . \n","The campaign leading to the election was not so quiet , however . \n","It was marked by controversy , anonymous midnight phone calls and veiled threats of violence . \n","The former county school superintendent , George P. Callan , shot himself to death March 18 , four days after he resigned his post in a dispute with the county school board . \n","During the election campaign , both candidates , Davis and Bush , reportedly received anonymous telephone calls . \n","Ordinary Williams said he , too , was subjected to anonymous calls soon after he scheduled the election . \n","Many local citizens feared that there would be irregularities at the polls , and Williams got himself a permit to carry a gun and promised an orderly election . \n","Sheriff Felix Tabb said the ordinary apparently made good his promise . \n","\" Everything went real smooth \" , the sheriff said . \n","\" There was n't a bit of trouble \" . \n","\n"]}],"source":["#Make df_ref dataset \n","df_ref = makeDataset(selected_dataset)"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["df = pd.concat([df_Q,df_K1, df_K2, df_ref])\n","df = df.reset_index(drop=True)\n","del df_Q, df_K1,df_K2,df_ref"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>body</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Oslo The most positive element to emerge from ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                               body\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...\n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...\n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...\n","3  BROWN DATASET  Oslo The most positive element to emerge from ..."]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["# 1. count, list and order the frequency of words \n","\n","#Feature1"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"vaALdJiudYTX"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","def tokenizeFunc(documents):\n","    # documents = df['body'].tolist()\n","    tf_vectorizer = CountVectorizer()\n","    tf_vectors = tf_vectorizer.fit_transform(documents)         # word frequency list\n","    return tf_vectors, tf_vectorizer\n","# for i, msg in enumerate(df['body']):\n","#     # print(i, msg)\n","#     if msg is np.nan:\n","#         print(i, msg)\n","#tfidf_vectorizer = TfidfVectorizer()\n","#tfidf_vectors = tfidf_vectorizer.fit_transform(documents) # keyword frequency list\n","# tf_vectorizer = CountVectorizer()\n","# tf_vectors = tf_vectorizer.fit_transform(documents)         # word frequency list\n","# del tf_vectorizer\n","# del tfidf_vectorizer# データ分割r"]},{"cell_type":"markdown","metadata":{},"source":["### Make All datasets a list and make tf vector and tf vectrizer"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["documents=[\n","    df['body'][0], #df_Q['body'][0],\n","    df['body'][1], #df_K2['body'][0],\n","    df['body'][2], #df_K2['body'][0],\n","    df['body'][3], #df_ref['body'][0],]\n","]\n","tf_vectors, tf_vectorizer = tokenizeFunc(documents)\n"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["#[remove]Dataframeや単語リストが一つのDFで十分な場合削除　7/25日米田\n","\n","# tf_vectors_Q, tf_vectorizer_Q = tokenizeFunc(df_Q)\n","# tf_vectors_K2, tf_vectorizer_K1 = tokenizeFunc(df_K1)\n","# tf_vectors_K2, tf_vectorizer_K2 = tokenizeFunc(df_K2)\n","# tf_vectors_ref, tf_vectorizer_ref = tokenizeFunc(df_ref)\n","\n","# words_Q=tf_vectorizer_Q.get_feature_names_out()\n","# words_K1=tf_vectorizer_K1.get_feature_names_out()\n","# words_K2=tf_vectorizer_K2.get_feature_names_out()\n","# words_K3=tf_vectorizer_ref.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{},"source":["## Make a words dictionary in all documents "]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1689127987186,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"mblTowsDdYTg","outputId":"d0e28c64-bf6a-46f4-f4b3-930bed4f76a8"},"outputs":[],"source":["# 作成された辞書を作る　:トレインデータ・テストデータ両方に対応\n","words=tf_vectorizer.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{},"source":["## Make the Words frequency matrix "]},{"cell_type":"markdown","metadata":{},"source":["### This Matrix's row indices ared corresponding with a document in the "]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["#First row: dataset Q\n","#Second row: dataset K1\n","#Third row: dataset k2\n","#Fourth row: dataset ref"]},{"cell_type":"markdown","metadata":{},"source":["### This Matrix's col indices are corresponding with a word in the above document"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"data":{"text/plain":["array(['000', '0072', '01', ..., 'znamenny', 'µm', 'μm'], dtype=object)"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["words"]},{"cell_type":"markdown","metadata":{},"source":["## Insert Words Frequemcy Vector into 'tf' Column in DF for Each Dataset"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["tf_mat = tf_vectors.toarray()\n","del tf_vectors\n","df['tf'] = tf_mat.tolist()"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>tf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 0, 0, 1, 6...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Oslo The most positive element to emerge from ...</td>\n","      <td>[12, 0, 0, 9, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                               body  \\\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","3  BROWN DATASET  Oslo The most positive element to emerge from ...   \n","\n","                                                  tf  \n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...  \n","1  [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...  \n","2  [2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 0, 0, 1, 6...  \n","3  [12, 0, 0, 9, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0,...  "]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"WNCL7tHudYTh"},"outputs":[],"source":["#[remove]ここの記述で必要な部分は上記に記述。そのた不要であれば削除\n","\n","#tfidf_mat = tfidf_vectors.toarray() # dead every time\n","#del tfidf_vectors\n","# tf_mat = tf_vectors.toarray()\n","# del tf_vectors\n","# df['tf'] = tf_mat.tolist()\n","#df['tfidf'] = tfidf_mat.tolist()"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"ZSGL6Vbkngv5"},"outputs":[],"source":["#[remove]今回はからのデータセットが存在しないため。確認後削除\n","\n","# 0 ベクトルを消去 Normalization のため\n","# for i, vec in enumerate(df['tf']):\n","#     if sum(vec) == 0:\n","#         df = df.drop(i)\n","\n","# df = df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. display the first 20 words of each dataset \n","\n","#Feature5"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["def dispAndMakeWordFreq(df, words, author = 0):\n","    data = df.loc[author]\n","    freq = data['tf']\n","    wf = pd.DataFrame({'words': words, 'frequency': freq})\n","    wordli = []\n","    freqli =[]\n","    wordindexli = []\n","    for key, data in wf.iterrows():\n","        if(int(data[1]) != 0):\n","            wordli.append(data[0])\n","            freqli.append(data[1])\n","            wordindexli.append(key)\n","        \n","    return pd.DataFrame({'words': wordli, 'frequency': freqli, 'wordIndex': wordindexli})\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset Q"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>186</td>\n","      <td>3481</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>89</td>\n","      <td>2414</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>74</td>\n","      <td>344</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>70</td>\n","      <td>3531</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1522</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in</td>\n","      <td>54</td>\n","      <td>1833</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>be</td>\n","      <td>45</td>\n","      <td>508</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>40</td>\n","      <td>2467</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>planning</td>\n","      <td>33</td>\n","      <td>2616</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>31</td>\n","      <td>1936</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>are</td>\n","      <td>31</td>\n","      <td>393</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>as</td>\n","      <td>28</td>\n","      <td>410</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>time</td>\n","      <td>28</td>\n","      <td>3523</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>location</td>\n","      <td>27</td>\n","      <td>2103</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>user</td>\n","      <td>26</td>\n","      <td>3651</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>based</td>\n","      <td>20</td>\n","      <td>503</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>that</td>\n","      <td>20</td>\n","      <td>3480</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>system</td>\n","      <td>19</td>\n","      <td>3411</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>such</td>\n","      <td>18</td>\n","      <td>3366</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>figure</td>\n","      <td>18</td>\n","      <td>1473</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       words  frequency  wordIndex\n","0        the        186       3481\n","1         of         89       2414\n","2        and         74        344\n","3         to         70       3531\n","4        for         55       1522\n","5         in         54       1833\n","6         be         45        508\n","7         or         40       2467\n","8   planning         33       2616\n","9         is         31       1936\n","10       are         31        393\n","11        as         28        410\n","12      time         28       3523\n","13  location         27       2103\n","14      user         26       3651\n","15     based         20        503\n","16      that         20       3480\n","17    system         19       3411\n","18      such         18       3366\n","19    figure         18       1473"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_Q = dispAndMakeWordFreq(df,words, author = 0)\n","wf_list_Q = wf_list_Q.sort_values('frequency', ascending=False)\n","wf_list_Q = wf_list_Q.reset_index(drop=True)\n","wf_list_Q.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset K1"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>78</td>\n","      <td>3481</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>66</td>\n","      <td>2414</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>50</td>\n","      <td>344</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in</td>\n","      <td>42</td>\n","      <td>1833</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>to</td>\n","      <td>39</td>\n","      <td>3531</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>as</td>\n","      <td>32</td>\n","      <td>410</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>music</td>\n","      <td>25</td>\n","      <td>2303</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>for</td>\n","      <td>17</td>\n","      <td>1522</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>is</td>\n","      <td>17</td>\n","      <td>1936</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>be</td>\n","      <td>16</td>\n","      <td>508</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>on</td>\n","      <td>15</td>\n","      <td>2429</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>tchaikovsky</td>\n","      <td>13</td>\n","      <td>3435</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>by</td>\n","      <td>13</td>\n","      <td>599</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>that</td>\n","      <td>12</td>\n","      <td>3480</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>can</td>\n","      <td>12</td>\n","      <td>615</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>not</td>\n","      <td>11</td>\n","      <td>2375</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>between</td>\n","      <td>11</td>\n","      <td>543</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>such</td>\n","      <td>11</td>\n","      <td>3366</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>similarity</td>\n","      <td>11</td>\n","      <td>3188</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>models</td>\n","      <td>10</td>\n","      <td>2255</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words  frequency  wordIndex\n","0           the         78       3481\n","1            of         66       2414\n","2           and         50        344\n","3            in         42       1833\n","4            to         39       3531\n","5            as         32        410\n","6         music         25       2303\n","7           for         17       1522\n","8            is         17       1936\n","9            be         16        508\n","10           on         15       2429\n","11  tchaikovsky         13       3435\n","12           by         13        599\n","13         that         12       3480\n","14          can         12        615\n","15          not         11       2375\n","16      between         11        543\n","17         such         11       3366\n","18   similarity         11       3188\n","19       models         10       2255"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_K1 = dispAndMakeWordFreq(df,words, author = 1)\n","wf_list_K1 = wf_list_K1.sort_values('frequency', ascending=False)\n","wf_list_K1 = wf_list_K1.reset_index(drop=True)\n","wf_list_K1.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset K2"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>383</td>\n","      <td>3481</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>161</td>\n","      <td>2414</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>to</td>\n","      <td>128</td>\n","      <td>3531</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>and</td>\n","      <td>121</td>\n","      <td>344</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>in</td>\n","      <td>119</td>\n","      <td>1833</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>is</td>\n","      <td>91</td>\n","      <td>1936</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>high</td>\n","      <td>76</td>\n","      <td>1735</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>eo</td>\n","      <td>56</td>\n","      <td>1316</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>with</td>\n","      <td>56</td>\n","      <td>3792</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1522</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>temperature</td>\n","      <td>53</td>\n","      <td>3452</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>at</td>\n","      <td>50</td>\n","      <td>437</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>gbit</td>\n","      <td>46</td>\n","      <td>1595</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>modulator</td>\n","      <td>45</td>\n","      <td>2265</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>device</td>\n","      <td>38</td>\n","      <td>1087</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>are</td>\n","      <td>37</td>\n","      <td>393</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>sph</td>\n","      <td>36</td>\n","      <td>3271</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>as</td>\n","      <td>36</td>\n","      <td>410</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>polymer</td>\n","      <td>31</td>\n","      <td>2650</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>up</td>\n","      <td>29</td>\n","      <td>3637</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words  frequency  wordIndex\n","0           the        383       3481\n","1            of        161       2414\n","2            to        128       3531\n","3           and        121        344\n","4            in        119       1833\n","5            is         91       1936\n","6          high         76       1735\n","7            eo         56       1316\n","8          with         56       3792\n","9           for         55       1522\n","10  temperature         53       3452\n","11           at         50        437\n","12         gbit         46       1595\n","13    modulator         45       2265\n","14       device         38       1087\n","15          are         37        393\n","16          sph         36       3271\n","17           as         36        410\n","18      polymer         31       2650\n","19           up         29       3637"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_K2 = dispAndMakeWordFreq(df,words, author = 2)\n","\n","wf_list_K2 = wf_list_K2.sort_values('frequency', ascending=False)\n","wf_list_K2 = wf_list_K2.reset_index(drop=True)\n","wf_list_K2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset ref"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>598</td>\n","      <td>3481</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>292</td>\n","      <td>2414</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>to</td>\n","      <td>225</td>\n","      <td>3531</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in</td>\n","      <td>176</td>\n","      <td>1833</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>and</td>\n","      <td>158</td>\n","      <td>344</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>for</td>\n","      <td>101</td>\n","      <td>1522</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>that</td>\n","      <td>84</td>\n","      <td>3480</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>by</td>\n","      <td>67</td>\n","      <td>599</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>would</td>\n","      <td>65</td>\n","      <td>3810</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>on</td>\n","      <td>63</td>\n","      <td>2429</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>was</td>\n","      <td>58</td>\n","      <td>3731</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>said</td>\n","      <td>58</td>\n","      <td>3044</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>be</td>\n","      <td>55</td>\n","      <td>508</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>it</td>\n","      <td>53</td>\n","      <td>1942</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>he</td>\n","      <td>51</td>\n","      <td>1714</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>as</td>\n","      <td>51</td>\n","      <td>410</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>is</td>\n","      <td>49</td>\n","      <td>1936</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>has</td>\n","      <td>40</td>\n","      <td>1707</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>this</td>\n","      <td>37</td>\n","      <td>3506</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>his</td>\n","      <td>33</td>\n","      <td>1746</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    words  frequency  wordIndex\n","0     the        598       3481\n","1      of        292       2414\n","2      to        225       3531\n","3      in        176       1833\n","4     and        158        344\n","5     for        101       1522\n","6    that         84       3480\n","7      by         67        599\n","8   would         65       3810\n","9      on         63       2429\n","10    was         58       3731\n","11   said         58       3044\n","12     be         55        508\n","13     it         53       1942\n","14     he         51       1714\n","15     as         51        410\n","16     is         49       1936\n","17    has         40       1707\n","18   this         37       3506\n","19    his         33       1746"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_ref = dispAndMakeWordFreq(df,words, author = 3)\n","wf_list_ref = wf_list_ref.sort_values('frequency', ascending=False)\n","wf_list_ref = wf_list_ref.reset_index(drop=True)\n","wf_list_ref.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## count, list and order the frequency of keywords\n","#Feature2"]},{"cell_type":"markdown","metadata":{},"source":["## Normalization of Word Frequencies to All Datasets and Add them into DF"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["# we generaly name 'ntf' for normalized term frequency\n","normalized_tf_list = []\n","for row in df['tf']:\n","    num_words = sum(row)\n","    normalized_tf = []\n","    for x in row:\n","        normalized_tf.append(x/num_words)\n","    normalized_tf_list.append(normalized_tf)\n","\n","df['ntf'] = normalized_tf_list"]},{"cell_type":"markdown","metadata":{},"source":["#Tf-idf の代わりに利用する keyness を作る\n","\n","ここでは　df['keyness'] を作成し追加したい"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["import math\n","# we generaly name 'ntf' for normalized term frequency\n","# First, create the shared normalized tf vector\n","# shared_ntf = None # shared ntf of all document\n","# matrix = []\n","# for row in df['ntf']:\n","#     matrix.append(row)\n","# np_matrix = np.array(matrix)\n","# mean_vector = np_matrix.mean(axis=0)\n","# shared_ntf = mean_vector.tolist()\n","\n","def keyness(ntf_vector1, ref_ntf_vector2): # freq_vector1 and freq_vector2 are both already normalized\n","    keyness_vec = []\n","    for i, x in enumerate(ntf_vector1):\n","        if ntf_vector1[i] == 0 or ref_ntf_vector2[i] == 0:\n","            keyness_vec.append(0)\n","        else:\n","            keyness_vec.append(math.log2(ntf_vector1[i]/ref_ntf_vector2[i]))\n","\n","    return keyness_vec\n","\n","\n","keyness_mat = []\n","for ntf_vector in df['ntf']:\n","    ntf_ref = df['ntf'][3]\n","    keyness_vec = keyness(ntf_vector, ntf_ref)\n","    keyness_mat.append(keyness_vec)\n","\n","# keyness を　Dataframe に追加\n","df['keyness'] = keyness_mat"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....</td>\n","      <td>[0, 0, 0, -0.7214760325456605, 0, 0, 0, 0, 1.4...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...</td>\n","      <td>[0, 0, 0, -0.8249341252296036, 1.3449908762127...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 0, 0, 1, 6...</td>\n","      <td>[0.0004100041000410004, 0.0002050020500205002,...</td>\n","      <td>[-1.9042681509764212, 0, 0, 0.3181242703600269...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Oslo The most positive element to emerge from ...</td>\n","      <td>[12, 0, 0, 9, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0,...</td>\n","      <td>[0.0015347231103721704, 0.0, 0.0, 0.0011510423...</td>\n","      <td>[0.0, 0, 0, 0.0, 0.0, 0, 0, 0, 0.0, 0, 0, 0.0,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                               body  \\\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","3  BROWN DATASET  Oslo The most positive element to emerge from ...   \n","\n","                                                  tf  \\\n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","1  [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","2  [2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 0, 0, 1, 6...   \n","3  [12, 0, 0, 9, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0,...   \n","\n","                                                 ntf  \\\n","0  [0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....   \n","1  [0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...   \n","2  [0.0004100041000410004, 0.0002050020500205002,...   \n","3  [0.0015347231103721704, 0.0, 0.0, 0.0011510423...   \n","\n","                                             keyness  \n","0  [0, 0, 0, -0.7214760325456605, 0, 0, 0, 0, 1.4...  \n","1  [0, 0, 0, -0.8249341252296036, 1.3449908762127...  \n","2  [-1.9042681509764212, 0, 0, 0.3181242703600269...  \n","3  [0.0, 0, 0, 0.0, 0.0, 0, 0, 0, 0.0, 0, 0, 0.0,...  "]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["# Display the first 20 keywords of each dataset "]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["def dispAndMakeKeyWordList(df, words, author = 0):\n","    data = df.loc[author]\n","    keyness = data['keyness']\n","    wf = pd.DataFrame({'words': words, 'keyness': keyness})\n","    wordli = []\n","    freqli =[]\n","    wordindexli = []\n","    for key, data in wf.iterrows():\n","        if(int(data[1]) != 0):\n","            wordli.append(data[0])\n","            freqli.append(data[1])\n","            wordindexli.append(key)\n","        \n","    return pd.DataFrame({'words': wordli, 'keyness': freqli, 'wordIndex': wordindexli})"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword of Dataset Q"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>planning</td>\n","      <td>6.492843</td>\n","      <td>2616</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>based</td>\n","      <td>5.770377</td>\n","      <td>503</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>project</td>\n","      <td>4.770377</td>\n","      <td>2753</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>might</td>\n","      <td>4.770377</td>\n","      <td>2227</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>major</td>\n","      <td>4.448449</td>\n","      <td>2131</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>current</td>\n","      <td>4.448449</td>\n","      <td>953</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>management</td>\n","      <td>4.255804</td>\n","      <td>2139</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>specific</td>\n","      <td>4.255804</td>\n","      <td>3263</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>advance</td>\n","      <td>4.255804</td>\n","      <td>243</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>setting</td>\n","      <td>4.033411</td>\n","      <td>3145</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>suggested</td>\n","      <td>4.033411</td>\n","      <td>3370</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>visiting</td>\n","      <td>3.770377</td>\n","      <td>3699</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>sense</td>\n","      <td>3.770377</td>\n","      <td>3121</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>requirements</td>\n","      <td>3.770377</td>\n","      <td>2954</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>frame</td>\n","      <td>3.770377</td>\n","      <td>1547</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>learned</td>\n","      <td>3.770377</td>\n","      <td>2037</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>while</td>\n","      <td>3.563926</td>\n","      <td>3771</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>different</td>\n","      <td>3.448449</td>\n","      <td>1100</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>domain</td>\n","      <td>3.448449</td>\n","      <td>1168</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>thus</td>\n","      <td>3.448449</td>\n","      <td>3519</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           words   keyness  wordIndex\n","0       planning  6.492843       2616\n","1          based  5.770377        503\n","2        project  4.770377       2753\n","3          might  4.770377       2227\n","4          major  4.448449       2131\n","5        current  4.448449        953\n","6     management  4.255804       2139\n","7       specific  4.255804       3263\n","8        advance  4.255804        243\n","9        setting  4.033411       3145\n","10     suggested  4.033411       3370\n","11      visiting  3.770377       3699\n","12         sense  3.770377       3121\n","13  requirements  3.770377       2954\n","14         frame  3.770377       1547\n","15       learned  3.770377       2037\n","16         while  3.563926       3771\n","17     different  3.448449       1100\n","18        domain  3.448449       1168\n","19          thus  3.448449       3519"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["keyword_list_Q = dispAndMakeKeyWordList(df,words, author = 0)\n","\n","keyword_list_Q = keyword_list_Q.sort_values('keyness',ascending=False)\n","keyword_list_Q = keyword_list_Q.reset_index(drop=True)\n","keyword_list_Q.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword of Dataset K1"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>4.929953</td>\n","      <td>503</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>analysis</td>\n","      <td>4.666919</td>\n","      <td>341</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cope</td>\n","      <td>4.344991</td>\n","      <td>894</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>elements</td>\n","      <td>4.344991</td>\n","      <td>1252</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>different</td>\n","      <td>3.929953</td>\n","      <td>1100</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>whole</td>\n","      <td>3.929953</td>\n","      <td>3775</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>mostly</td>\n","      <td>3.929953</td>\n","      <td>2284</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>composition</td>\n","      <td>3.929953</td>\n","      <td>784</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>well</td>\n","      <td>3.929953</td>\n","      <td>3757</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>tension</td>\n","      <td>3.929953</td>\n","      <td>3459</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>learning</td>\n","      <td>3.344991</td>\n","      <td>2038</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>groups</td>\n","      <td>3.344991</td>\n","      <td>1668</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>existing</td>\n","      <td>3.344991</td>\n","      <td>1375</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>according</td>\n","      <td>3.344991</td>\n","      <td>194</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>see</td>\n","      <td>3.344991</td>\n","      <td>3101</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>within</td>\n","      <td>3.344991</td>\n","      <td>3793</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>recent</td>\n","      <td>3.344991</td>\n","      <td>2859</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>between</td>\n","      <td>3.219460</td>\n","      <td>543</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>children</td>\n","      <td>3.192988</td>\n","      <td>675</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>rules</td>\n","      <td>2.929953</td>\n","      <td>3033</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words   keyness  wordIndex\n","0         based  4.929953        503\n","1      analysis  4.666919        341\n","2          cope  4.344991        894\n","3      elements  4.344991       1252\n","4     different  3.929953       1100\n","5         whole  3.929953       3775\n","6        mostly  3.929953       2284\n","7   composition  3.929953        784\n","8          well  3.929953       3757\n","9       tension  3.929953       3459\n","10     learning  3.344991       2038\n","11       groups  3.344991       1668\n","12     existing  3.344991       1375\n","13    according  3.344991        194\n","14          see  3.344991       3101\n","15       within  3.344991       3793\n","16       recent  3.344991       2859\n","17      between  3.219460        543\n","18     children  3.192988        675\n","19        rules  2.929953       3033"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["keyword_list_K1 = dispAndMakeKeyWordList(df,words, author = 1)\n","keyword_list_K1 = keyword_list_K1.sort_values('keyness',ascending=False)\n","keyword_list_K1 = keyword_list_K1.reset_index(drop=True)\n","keyword_list_K1.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword of Dataset K2"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>energy</td>\n","      <td>5.488049</td>\n","      <td>1288</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>speed</td>\n","      <td>5.381134</td>\n","      <td>3269</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>200</td>\n","      <td>4.928622</td>\n","      <td>58</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>high</td>\n","      <td>4.606694</td>\n","      <td>1735</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>efficiency</td>\n","      <td>4.587585</td>\n","      <td>1229</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>power</td>\n","      <td>4.265657</td>\n","      <td>2671</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>operating</td>\n","      <td>4.140126</td>\n","      <td>2445</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>100</td>\n","      <td>4.140126</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>signal</td>\n","      <td>4.002622</td>\n","      <td>3178</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>range</td>\n","      <td>3.850619</td>\n","      <td>2826</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>thus</td>\n","      <td>3.850619</td>\n","      <td>3519</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>bit</td>\n","      <td>3.680694</td>\n","      <td>552</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>storage</td>\n","      <td>3.488049</td>\n","      <td>3318</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>around</td>\n","      <td>3.488049</td>\n","      <td>404</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>factor</td>\n","      <td>3.488049</td>\n","      <td>1429</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>operation</td>\n","      <td>3.488049</td>\n","      <td>2446</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>period</td>\n","      <td>3.488049</td>\n","      <td>2566</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>90</td>\n","      <td>3.488049</td>\n","      <td>166</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>side</td>\n","      <td>3.488049</td>\n","      <td>3174</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>extremely</td>\n","      <td>3.265657</td>\n","      <td>1413</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         words   keyness  wordIndex\n","0       energy  5.488049       1288\n","1        speed  5.381134       3269\n","2          200  4.928622         58\n","3         high  4.606694       1735\n","4   efficiency  4.587585       1229\n","5        power  4.265657       2671\n","6    operating  4.140126       2445\n","7          100  4.140126          4\n","8       signal  4.002622       3178\n","9        range  3.850619       2826\n","10        thus  3.850619       3519\n","11         bit  3.680694        552\n","12     storage  3.488049       3318\n","13      around  3.488049        404\n","14      factor  3.488049       1429\n","15   operation  3.488049       2446\n","16      period  3.488049       2566\n","17          90  3.488049        166\n","18        side  3.488049       3174\n","19   extremely  3.265657       1413"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["keyword_list_K2 = dispAndMakeKeyWordList(df,words, author = 2)\n","\n","keyword_list_K2 = keyword_list_K2.sort_values('keyness',ascending=False)\n","keyword_list_K2 = keyword_list_K2.reset_index(drop=True)\n","keyword_list_K2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["# 7. display the shared words in the first 20 words of each dataset\n","#Feature7"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["def dispAndMakeSharedWordsFreq(df,df_ref):\n","    df = df[df['words'].isin(df_ref['words'])] #filtering with the words in df2\n","    df_ref = df_ref[df_ref['words'].isin(df['words'])] #filtering with the words in df\n","    #now the words in df and df2 are same\n","    #sort words in the alphabetical order to become the same words as the same rows\n","    df = df.sort_values('words')\n","    df_ref = df_ref.sort_values('words')\n","    #merge df2 frequency to df1 \n","    df['ref_frequency'] = list(df_ref['frequency'])\n","    df['shared_word_keyword_frequency'] = (df['frequency'] + df['ref_frequency'])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Words Frequency in Dataset Q and K1"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","      <th>ref_frequency</th>\n","      <th>shared_word_keyword_frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>186</td>\n","      <td>3481</td>\n","      <td>78</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>89</td>\n","      <td>2414</td>\n","      <td>66</td>\n","      <td>155</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>74</td>\n","      <td>344</td>\n","      <td>50</td>\n","      <td>124</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>70</td>\n","      <td>3531</td>\n","      <td>39</td>\n","      <td>109</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1522</td>\n","      <td>17</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in</td>\n","      <td>54</td>\n","      <td>1833</td>\n","      <td>42</td>\n","      <td>96</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>be</td>\n","      <td>45</td>\n","      <td>508</td>\n","      <td>16</td>\n","      <td>61</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>40</td>\n","      <td>2467</td>\n","      <td>5</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>is</td>\n","      <td>31</td>\n","      <td>1936</td>\n","      <td>17</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>are</td>\n","      <td>31</td>\n","      <td>393</td>\n","      <td>4</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>as</td>\n","      <td>28</td>\n","      <td>410</td>\n","      <td>32</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>time</td>\n","      <td>28</td>\n","      <td>3523</td>\n","      <td>1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>based</td>\n","      <td>20</td>\n","      <td>503</td>\n","      <td>6</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>that</td>\n","      <td>20</td>\n","      <td>3480</td>\n","      <td>12</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>system</td>\n","      <td>19</td>\n","      <td>3411</td>\n","      <td>3</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>by</td>\n","      <td>18</td>\n","      <td>599</td>\n","      <td>13</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>such</td>\n","      <td>18</td>\n","      <td>3366</td>\n","      <td>11</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>on</td>\n","      <td>18</td>\n","      <td>2429</td>\n","      <td>15</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>figure</td>\n","      <td>18</td>\n","      <td>1473</td>\n","      <td>9</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>with</td>\n","      <td>17</td>\n","      <td>3792</td>\n","      <td>9</td>\n","      <td>26</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     words  frequency  wordIndex  ref_frequency  shared_word_keyword_frequency\n","0      the        186       3481             78                            264\n","1       of         89       2414             66                            155\n","2      and         74        344             50                            124\n","3       to         70       3531             39                            109\n","4      for         55       1522             17                             72\n","5       in         54       1833             42                             96\n","6       be         45        508             16                             61\n","7       or         40       2467              5                             45\n","8       is         31       1936             17                             48\n","9      are         31        393              4                             35\n","10      as         28        410             32                             60\n","11    time         28       3523              1                             29\n","12   based         20        503              6                             26\n","13    that         20       3480             12                             32\n","14  system         19       3411              3                             22\n","15      by         18        599             13                             31\n","16    such         18       3366             11                             29\n","17      on         18       2429             15                             33\n","18  figure         18       1473              9                             27\n","19    with         17       3792              9                             26"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["#SWF = Shared Word Frequency\n","SWF_QandK1 = dispAndMakeSharedWordsFreq(wf_list_Q,wf_list_K1)\n","SWF_QandK1=SWF_QandK1.sort_values('frequency', ascending=False)\n","SWF_QandK1 = SWF_QandK1.reset_index(drop=True)\n","SWF_QandK1.head(20)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Words Frequency in Dataset Q and K2"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","      <th>ref_frequency</th>\n","      <th>shared_word_keyword_frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>186</td>\n","      <td>3481</td>\n","      <td>383</td>\n","      <td>569</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>89</td>\n","      <td>2414</td>\n","      <td>161</td>\n","      <td>250</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>74</td>\n","      <td>344</td>\n","      <td>121</td>\n","      <td>195</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>70</td>\n","      <td>3531</td>\n","      <td>128</td>\n","      <td>198</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1522</td>\n","      <td>55</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in</td>\n","      <td>54</td>\n","      <td>1833</td>\n","      <td>119</td>\n","      <td>173</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>be</td>\n","      <td>45</td>\n","      <td>508</td>\n","      <td>19</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>40</td>\n","      <td>2467</td>\n","      <td>10</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>is</td>\n","      <td>31</td>\n","      <td>1936</td>\n","      <td>91</td>\n","      <td>122</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>are</td>\n","      <td>31</td>\n","      <td>393</td>\n","      <td>37</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>time</td>\n","      <td>28</td>\n","      <td>3523</td>\n","      <td>1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>as</td>\n","      <td>28</td>\n","      <td>410</td>\n","      <td>36</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>based</td>\n","      <td>20</td>\n","      <td>503</td>\n","      <td>3</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>that</td>\n","      <td>20</td>\n","      <td>3480</td>\n","      <td>7</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>system</td>\n","      <td>19</td>\n","      <td>3411</td>\n","      <td>3</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>figure</td>\n","      <td>18</td>\n","      <td>1473</td>\n","      <td>5</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>by</td>\n","      <td>18</td>\n","      <td>599</td>\n","      <td>26</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>such</td>\n","      <td>18</td>\n","      <td>3366</td>\n","      <td>16</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>on</td>\n","      <td>18</td>\n","      <td>2429</td>\n","      <td>26</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>it</td>\n","      <td>17</td>\n","      <td>1942</td>\n","      <td>10</td>\n","      <td>27</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     words  frequency  wordIndex  ref_frequency  shared_word_keyword_frequency\n","0      the        186       3481            383                            569\n","1       of         89       2414            161                            250\n","2      and         74        344            121                            195\n","3       to         70       3531            128                            198\n","4      for         55       1522             55                            110\n","5       in         54       1833            119                            173\n","6       be         45        508             19                             64\n","7       or         40       2467             10                             50\n","8       is         31       1936             91                            122\n","9      are         31        393             37                             68\n","10    time         28       3523              1                             29\n","11      as         28        410             36                             64\n","12   based         20        503              3                             23\n","13    that         20       3480              7                             27\n","14  system         19       3411              3                             22\n","15  figure         18       1473              5                             23\n","16      by         18        599             26                             44\n","17    such         18       3366             16                             34\n","18      on         18       2429             26                             44\n","19      it         17       1942             10                             27"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["#SWF = Shared Word Frequency\n","SWF_QandK2 = dispAndMakeSharedWordsFreq(wf_list_Q,wf_list_K2)\n","SWF_QandK2 = SWF_QandK2.sort_values('frequency', ascending=False)\n","SWF_QandK2 = SWF_QandK2.reset_index(drop=True)\n","SWF_QandK2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Display the shared keywords in the first 20 keywords of each dataset"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["def dispAndMakeSharedKeyword(df,df_ref):\n","    df = df[df['words'].isin(df_ref['words'])] #filtering with the words in df2\n","    df_ref = df_ref[df_ref['words'].isin(df['words'])] #filtering with the words in df\n","    #now the words in df and df2 are same\n","    #sort words in the alphabetical order to become the same words as the same rows\n","    df = df.sort_values('words')\n","    df_ref = df_ref.sort_values('words')\n","    #merge df2 frequency to df1 \n","    df['ref_keyness'] = list(df_ref['keyness'])\n","    df['wordIndex'] = list(df['wordIndex'])\n","    \n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Keywords in Dataset Q and K1"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","      <th>ref_keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>5.770377</td>\n","      <td>503</td>\n","      <td>4.929953</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>project</td>\n","      <td>4.770377</td>\n","      <td>2753</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>current</td>\n","      <td>4.448449</td>\n","      <td>953</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>suggested</td>\n","      <td>4.033411</td>\n","      <td>3370</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>frame</td>\n","      <td>3.770377</td>\n","      <td>1547</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>thus</td>\n","      <td>3.448449</td>\n","      <td>3519</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>important</td>\n","      <td>3.448449</td>\n","      <td>1824</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>different</td>\n","      <td>3.448449</td>\n","      <td>1100</td>\n","      <td>3.929953</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>system</td>\n","      <td>3.374448</td>\n","      <td>3411</td>\n","      <td>1.608025</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>we</td>\n","      <td>3.255804</td>\n","      <td>3746</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>possible</td>\n","      <td>3.255804</td>\n","      <td>2669</td>\n","      <td>1.344991</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>used</td>\n","      <td>3.033411</td>\n","      <td>3648</td>\n","      <td>1.344991</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>use</td>\n","      <td>2.863486</td>\n","      <td>3647</td>\n","      <td>1.760028</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>may</td>\n","      <td>2.670841</td>\n","      <td>2173</td>\n","      <td>1.760028</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>main</td>\n","      <td>2.448449</td>\n","      <td>2127</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>following</td>\n","      <td>2.448449</td>\n","      <td>1516</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>range</td>\n","      <td>2.448449</td>\n","      <td>2826</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>information</td>\n","      <td>2.448449</td>\n","      <td>1871</td>\n","      <td>2.344991</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>existing</td>\n","      <td>2.448449</td>\n","      <td>1375</td>\n","      <td>3.344991</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>our</td>\n","      <td>2.448449</td>\n","      <td>2485</td>\n","      <td>2.344991</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words   keyness  wordIndex  ref_keyness\n","0         based  5.770377        503     4.929953\n","1       project  4.770377       2753     2.344991\n","2       current  4.448449        953     2.344991\n","3     suggested  4.033411       3370     2.344991\n","4         frame  3.770377       1547     2.344991\n","5          thus  3.448449       3519     2.344991\n","6     important  3.448449       1824     2.344991\n","7     different  3.448449       1100     3.929953\n","8        system  3.374448       3411     1.608025\n","9            we  3.255804       3746     2.344991\n","10     possible  3.255804       2669     1.344991\n","11         used  3.033411       3648     1.344991\n","12          use  2.863486       3647     1.760028\n","13          may  2.670841       2173     1.760028\n","14         main  2.448449       2127     2.344991\n","15    following  2.448449       1516     2.344991\n","16        range  2.448449       2826     2.344991\n","17  information  2.448449       1871     2.344991\n","18     existing  2.448449       1375     3.344991\n","19          our  2.448449       2485     2.344991"]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":["SK_QandK1 = dispAndMakeSharedKeyword(keyword_list_Q,keyword_list_K1)\n","SK_QandK1 = SK_QandK1.sort_values('keyness', ascending=False)\n","SK_QandK1 = SK_QandK1.reset_index(drop=True)\n","SK_QandK1.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Keywords in Dataset Q and K2"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","      <th>ref_keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>based</td>\n","      <td>5.770377</td>\n","      <td>503</td>\n","      <td>2.265657</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>requirements</td>\n","      <td>3.770377</td>\n","      <td>2954</td>\n","      <td>2.680694</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>thus</td>\n","      <td>3.448449</td>\n","      <td>3519</td>\n","      <td>3.850619</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>important</td>\n","      <td>3.448449</td>\n","      <td>1824</td>\n","      <td>2.265657</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>different</td>\n","      <td>3.448449</td>\n","      <td>1100</td>\n","      <td>1.680694</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>we</td>\n","      <td>3.255804</td>\n","      <td>3746</td>\n","      <td>1.265657</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>time</td>\n","      <td>3.085879</td>\n","      <td>3523</td>\n","      <td>-2.489231</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>used</td>\n","      <td>3.033411</td>\n","      <td>3648</td>\n","      <td>2.265657</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>side</td>\n","      <td>3.033411</td>\n","      <td>3174</td>\n","      <td>3.488049</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>may</td>\n","      <td>2.670841</td>\n","      <td>2173</td>\n","      <td>-1.904268</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>our</td>\n","      <td>2.448449</td>\n","      <td>2485</td>\n","      <td>2.417660</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>improve</td>\n","      <td>2.448449</td>\n","      <td>1829</td>\n","      <td>1.680694</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>improving</td>\n","      <td>2.448449</td>\n","      <td>1832</td>\n","      <td>1.680694</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>range</td>\n","      <td>2.448449</td>\n","      <td>2826</td>\n","      <td>3.850619</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>period</td>\n","      <td>2.448449</td>\n","      <td>2566</td>\n","      <td>3.488049</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>are</td>\n","      <td>2.315182</td>\n","      <td>393</td>\n","      <td>1.802685</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>such</td>\n","      <td>2.158942</td>\n","      <td>3366</td>\n","      <td>1.221263</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>can</td>\n","      <td>2.033411</td>\n","      <td>615</td>\n","      <td>-1.319306</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>better</td>\n","      <td>2.033411</td>\n","      <td>541</td>\n","      <td>-1.319306</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>could</td>\n","      <td>1.670841</td>\n","      <td>911</td>\n","      <td>2.002622</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           words   keyness  wordIndex  ref_keyness\n","0          based  5.770377        503     2.265657\n","1   requirements  3.770377       2954     2.680694\n","2           thus  3.448449       3519     3.850619\n","3      important  3.448449       1824     2.265657\n","4      different  3.448449       1100     1.680694\n","5             we  3.255804       3746     1.265657\n","6           time  3.085879       3523    -2.489231\n","7           used  3.033411       3648     2.265657\n","8           side  3.033411       3174     3.488049\n","9            may  2.670841       2173    -1.904268\n","10           our  2.448449       2485     2.417660\n","11       improve  2.448449       1829     1.680694\n","12     improving  2.448449       1832     1.680694\n","13         range  2.448449       2826     3.850619\n","14        period  2.448449       2566     3.488049\n","15           are  2.315182        393     1.802685\n","16          such  2.158942       3366     1.221263\n","17           can  2.033411        615    -1.319306\n","18        better  2.033411        541    -1.319306\n","19         could  1.670841        911     2.002622"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["SK_QandK2 = dispAndMakeSharedKeyword(keyword_list_Q,keyword_list_K2)\n","SK_QandK2 = SK_QandK2.sort_values('keyness', ascending=False)\n","SK_QandK2 = SK_QandK2.reset_index(drop=True)\n","SK_QandK2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["# Remake Keyness values and Shared Words Frequency for Each Author "]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["#Only filterling the keyness and SFW values using sheared word freq and shared keyword.\n","#this objective is to adjust the length of shared keyword and word list to original size of 'words'\n","def remakeKeynessAndFW(df, words,new_SWF,new_keyness, authorId = 0 ):\n","    if(authorId==0):\n","        print('dataset Q is not allowed')\n","        return [],[]\n","    #init list by 0\n","    keynessli=[]\n","    SFWli = []\n","    for k in range(len(words)):\n","        keynessli.append(0)\n","        SFWli.append(0)\n","    #end init\n","    #make new shared word frequency list\n","    #items[] compounds of: words,\tfrequency,\twordIndex,\tref_frequency\n","    for key,items in new_SWF.iterrows():\n","        SFWli[items[2]] = items[3]\n","    #make new shared keyword list\n","    # items[] compounds of: words,\tkeyness,\twordIndex,\tref_keyness\n","    for key, items in new_keyness.iterrows():\n","        keynessli[items[2]] = items[3]\n","    return SFWli, keynessli"]},{"cell_type":"markdown","metadata":{},"source":["### Now K1 and K2 in df have the tf and keyness based on the existence of each shared word freq. and shared keywords as new datasets"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["df['tf'][1], df['keyness'][1] = remakeKeynessAndFW(df, words, SWF_QandK1, SK_QandK1, 1)\n","df['tf'][2], df['keyness'][2] = remakeKeynessAndFW(df, words, SWF_QandK2, SK_QandK2, 2)"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....</td>\n","      <td>[0, 0, 0, -0.7214760325456605, 0, 0, 0, 0, 1.4...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 2.3449908762127087, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0004100041000410004, 0.0002050020500205002,...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Oslo The most positive element to emerge from ...</td>\n","      <td>[12, 0, 0, 9, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0,...</td>\n","      <td>[0.0015347231103721704, 0.0, 0.0, 0.0011510423...</td>\n","      <td>[0.0, 0, 0, 0.0, 0.0, 0, 0, 0, 0.0, 0, 0, 0.0,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                               body  \\\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","3  BROWN DATASET  Oslo The most positive element to emerge from ...   \n","\n","                                                  tf  \\\n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","1  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","2  [0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","3  [12, 0, 0, 9, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0,...   \n","\n","                                                 ntf  \\\n","0  [0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....   \n","1  [0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...   \n","2  [0.0004100041000410004, 0.0002050020500205002,...   \n","3  [0.0015347231103721704, 0.0, 0.0, 0.0011510423...   \n","\n","                                             keyness  \n","0  [0, 0, 0, -0.7214760325456605, 0, 0, 0, 0, 1.4...  \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 2.3449908762127087, 0...  \n","2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3  [0.0, 0, 0, 0.0, 0.0, 0, 0, 0, 0.0, 0, 0, 0.0,...  "]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"XYWb-aWPxIys"},"source":["# Prediction"]},{"cell_type":"code","execution_count":109,"metadata":{"id":"byj_PcU4dYTz"},"outputs":[],"source":["# # start からend までのwordの配列を返す\n","# def extract_features_words(freq_vector, words, start=0, end=20):\n","#     setX = set(freq_vector[0]) # 最大値を取り出すため set を作成\n","#     count = 0\n","#     result = []\n","#     while count<end:\n","#         max_value = max(setX)\n","#         max_index = freq_vector.index(max_value)\n","#         max_word = words[max_index]\n","#         setX.remove(max_value)\n","#         ### if exclude stopwords\n","#         # if max_word not in stop_words:\n","#         #     if count>= start:\n","#         #         result.append(max_index)\n","#         #     count+=1\n","#         if count>= start:\n","#             result.append(max_word)\n","#         count += 1\n","#     return result"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"6sUvSp9pdYT1"},"outputs":[],"source":["# start からend までのword IDの配列を返す\n","def extract_features(freq_vector, words, start=0, end=20):\n","    freq_vector = freq_vector[0].copy()\n","    setX = freq_vector # 最大値を取り出すため set を作成\n","    count = 0\n","    result=[]\n","    while count<end:\n","        try:\n","            max_value = max(setX)\n","            # print(max_value)\n","        except ValueError:\n","            print('valueerror')\n","            return result\n","        max_index = freq_vector.index(max_value)\n","        max_word = words[max_index]\n","        setX.remove(max_value)\n","        if count>= start:\n","            result.append(max_word)\n","        count += 1\n","        ### if exclude stopwords\n","        # if max_word not in stop_words:\n","        #     if count>= start:\n","        #         result.append(max_index)\n","        #     count+=1\n","\n","        \n","    return result\n","\n","#testcase\n","\n","# extract_features(df['tf'].tolist(), words, 0 , 20)"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"xWhTAjQYkwOy"},"outputs":[],"source":["def get_similarity(feature_vector1,feature_vector2):\n","    return len(set(feature_vector1) & set(feature_vector2))"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"BBTAic9Mdytb"},"outputs":[],"source":["INF = float('inf')\n","\n","def predict(questioned_vector,candidates_vectors):\n","    #initialize-------------------------------------\n","    start = 0\n","    end = 20\n","    similarityWithQ_tf = {}\n","    similarityWithQ_keyness = {}\n","    suspected = list(candidates_vectors['author'])\n","    \n","    #prepare questioned tf and keyword features\n","    Q_features_tf = extract_features(questioned_vector['tf'].tolist(), words, start, end)\n","    Q_features_keyness = extract_features(questioned_vector['keyness'].tolist(), words, start, end)\n","    while(len(suspected) > 1):\n","        for key, candidates_items in candidates_vectors.iterrows():\n","            author = candidates_items[0]\n","            \n","            #prepare candidates tf and keynes features\n","            candidates_vector_tf = [candidates_items[2]]\n","            candidates_vector_keyness = [candidates_items[4]]\n","            if author in suspected:\n","                print('Analysed Author Information')\n","                #tf-------------------\n","                C_features_tf = extract_features(candidates_vector_tf, words, start, end)\n","                score_tf = get_similarity(C_features_tf,Q_features_tf)\n","                similarityWithQ_tf[author]=score_tf\n","                print(f'{author}\\'s similality tf score = {score_tf}')\n","                \n","                #keyness----------------\n","                C_features_keyness = extract_features(candidates_vector_keyness, words, start, end)\n","                score_keyness = get_similarity(C_features_keyness,Q_features_keyness)\n","                similarityWithQ_keyness[author]=score_keyness\n","                print(f'{author}\\'s similality keyness score = {score_keyness}')\n","                print('...')\n","        innocent = min(similarityWithQ_keyness, key=similarityWithQ_keyness.get)\n","        suspect =  max(similarityWithQ_tf, key=similarityWithQ_tf.get)\n","\n","        #asking what they want to do\n","        act = 0\n","        while(1):\n","            if(len(suspected) == 1):\n","                print('*********************************')\n","                print(f'Final Result of the suspectful auther is: {suspect}')\n","                print('Thank you.')\n","                print('*********************************')\n","                return suspect\n","            print('*********************************')\n","            print(f'The MOST suspectful auther based on shared keyword frequency: {suspect}')\n","            print(f'The LEAST suspectful auther based on Keyness: {innocent}')\n","            print('*********************************')\n","            act = input(f'Do you wan to remove the LEAST suspectful one from searching, \\\"{innocent}\\\"?(yes: 1, no: 0)')\n","            try:\n","                if(int(act) == 1):\n","                    print('...')\n","                    print('remove the user from candidates')\n","                    suspected.remove(innocent)\n","                else:\n","                    print('...')\n","                    print('Go to next 20 words searching')\n","                    break\n","            except:\n","                print('Please input decimal number\\n')\n","                break\n","        start += 20\n","        end += 20\n","\n","    return suspected[0]"]},{"cell_type":"markdown","metadata":{},"source":["### Divided df into Q and Candidates"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....</td>\n","      <td>[0, 0, 0, -0.7214760325456605, 0, 0, 0, 0, 1.4...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      author                                               body  \\\n","0  Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","\n","                                                  tf  \\\n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","\n","                                                 ntf  \\\n","0  [0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....   \n","\n","                                             keyness  \n","0  [0, 0, 0, -0.7214760325456605, 0, 0, 0, 0, 1.4...  "]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["questioned_df = df[0:1].copy()\n","references_df = df[1:3].copy()\n","references_df.reset_index\n","questioned_df"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["# t = references_df['tf'][1]\n","# sum(t)"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 2.3449908762127087, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>[0.0004100041000410004, 0.0002050020500205002,...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       author                                               body  \\\n","1  K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2  K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","\n","                                                  tf  \\\n","1  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","2  [0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, ...   \n","\n","                                                 ntf  \\\n","1  [0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...   \n","2  [0.0004100041000410004, 0.0002050020500205002,...   \n","\n","                                             keyness  \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 2.3449908762127087, 0...  \n","2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["references_df"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Analysed Author Information\n","K1 DATASET's similality tf score = 4\n","K1 DATASET's similality keyness score = 2\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 6\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K2 DATASET\n","The LEAST suspectful auther based on Keyness: K2 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 2\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 1\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 1\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K2 DATASET\n","The LEAST suspectful auther based on Keyness: K2 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 1\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K2 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 1\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K2 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","Go to next 20 words searching\n","Analysed Author Information\n","K1 DATASET's similality tf score = 0\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 0\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","...\n","remove the user from candidates\n","*********************************\n","Final Result of the suspectful auther is: K1 DATASET\n","Thank you.\n","*********************************\n"]}],"source":["suspect = predict(questioned_df, references_df)\n","# suspect = predict(questioned_df, df)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 没キーワード頻度取得関数等"]},{"cell_type":"markdown","metadata":{"id":"BJVfCH7wy3LE"},"source":["## Check function in Train Data"]},{"cell_type":"code","execution_count":117,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1689128019130,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"1PsLfZIkxIyx","outputId":"6a3aaede-d05d-42ba-a843-7e031f4f2869"},"outputs":[{"ename":"NameError","evalue":"name 'reference_vectors' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[117], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m bad_guy \u001b[39m=\u001b[39m predict(df[\u001b[39m'\u001b[39m\u001b[39mkeyness\u001b[39m\u001b[39m'\u001b[39m][i], reference_vectors)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbad guy : \u001b[39m\u001b[39m{\u001b[39;00mbad_guy\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'reference_vectors' is not defined"]}],"source":["i = 0\n","bad_guy = predict(df['keyness'][i], reference_vectors)\n","print(f'bad guy : {bad_guy}')\n","print('-'*20)\n","print(f\"True author : {df['author'][i]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHK8sTcD8RSV"},"outputs":[],"source":["# all_test_data = len(X_keyness_test)# "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341225,"status":"ok","timestamp":1689129589788,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"Q3atvZGPkAP7","outputId":"87a4553b-1602-41c0-8292-f5fb7f16cd10"},"outputs":[],"source":["# match_cnt = 0\n","# all_test_data = len(X_keyness_train)\n","# for i in X_keyness_train.index:\n","#     bad_guy = predict(df['keyness'][i], reference_vectors)\n","#     if df['author'][i] == bad_guy:\n","#         match_cnt = match_cnt + 1\n","\n","# print(f'Math rate is: {match_cnt/all_test_data*100} % ')\n","#     #print(f'bad guy : {bad_guy}')\n","#     #print('-'*20)\n","#     #print(f\"True author : {df['author'][i]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7ltZakCdYT6"},"outputs":[],"source":["# def predict(questioned_vector, reference_vectors):\n","#     suspected = [author for author in authors]\n","\n","#     comparedSize = 20\n","#     while(len(suspected) > 1):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6PHDHBmdYT-"},"outputs":[],"source":["# def dispAndMakeSharedKeywordFreq(df,df_ref):\n","#     df = df[df['words'].isin(df_ref['words'])] #filtering with the words in df2\n","#     df_ref = df_ref[df_ref['words'].isin(df['words'])] #filtering with the words in df\n","#     #now the words in df and df2 are same\n","#     #sort words in the alphabetical order to become the same words as the same rows\n","#     df = df.sort_values('words')\n","#     df_ref = df_ref.sort_values('words')\n","#     #merge df2 frequency to df1 \n","#     df['ref_frequency'] = list(df_ref['frequency'])\n","#     df['shared_word_keyword_frequency'] = (df['frequency'] + df['ref_frequency'])\n","#     return df\n","## Keyword Frequency in Dataset Q and Ref\n","#KF = Keyword Frequency\n","# KF_QandRef = dispAndMakeSharedKeywordFreq(wf_list_Q,wf_list_ref)\n","# KF_QandRef.sort_values('frequency', ascending=False).head(20)\n","# ## Keyword Frequency in Dataset K1 and Ref\n","# #KF = Shared Keyword Frequency\n","# KF_QandRef = dispAndMakeSharedKeywordFreq(wf_list_K1,wf_list_ref)\n","# KF_QandRef.sort_values('frequency', ascending=False).head(20)\n","# ## Keyword Frequency in Dataset K2 and Ref\n","# #KF = Shared Keyword Frequency\n","# KF_QandRef = dispAndMakeSharedKeywordFreq(wf_list_K2,wf_list_ref)\n","# KF_QandRef.sort_values('frequency', ascending=False).head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import nltk\n","# from nltk.corpus import stopwords\n","\n","# nltk.download('stopwords')\n","# stop_words = stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #確認用\n","# i = 22\n","# msg = df['body'][i]\n","# max_value = max(keyness_mat[i])\n","# max_idx = keyness_mat[i].index(max_value)\n","# print(words[max_idx])\n","\n","# print(msg)\n","# #print(df['author'][i])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from sklearn.model_selection import train_test_split\n","# X_keyness_train, X_keyness_test, Y_keyness_train, Y_keyness_test = train_test_split(df['keyness'],df['author'],test_size=0.2,shuffle=True)\n","# X_tf_train, X_tf_test, Y_tf_train, Y_tf_test = train_test_split(df['tf'],df['author'],test_size=0.2,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # How many author?\n","# authors = set(Y_keyness_test)\n","# authors_list = [author for author in authors]"]},{"cell_type":"markdown","metadata":{},"source":["# Create Reference_vectors\n","size: 著者の数\n","\n","Train データから作る"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # df_X = pd.DataFrame(X_tf_train.values.tolist())\n","# # df_Y = pd.DataFrame(Y_tf_train.values.tolist())\n","\n","\n","# df_train = pd.concat((X_keyness_train, Y_keyness_train.rename('author')), axis=1)\n","# #\n","# reference_vectors = {}\n","# for author in authors:\n","\n","#     df_author = df_train.groupby('author').get_group(author)\n","\n","#     matrix = []\n","#     for row in df_author['keyness']:\n","#         matrix.append(row)\n","\n","#     np_matrix = np.array(matrix)\n","\n","#     mean_vector = np_matrix.mean(axis=0)\n","#     reference_vectors[author] = mean_vector.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for author, ref_vec in reference_vectors.items():\n","#     max_value = max(ref_vec)\n","#     max_idx =ref_vec.index(max_value)\n","#     print(f'{author}: {words[max_idx]}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# authors"]},{"cell_type":"markdown","metadata":{"id":"3IoM1rGcdYT5","notebookRunGroups":{"groupValue":"1"}},"source":["# Check function in Test Data"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"expsystem","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
